{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "913ea05e",
   "metadata": {},
   "source": [
    "# PageRank: Uma visão de Álgebra Linear\n",
    "\n",
    "### Introdução\n",
    "\n",
    "(Muitos dos quais lerão esse texto, raramente em sua vida, se depararam com o problema de ter que utilizar outro\n",
    "site de busca, além do Google, para encontrar algo que procura na internet. Porém, quando ela ainda estava em sua\n",
    "fase “jovem”, isso era um problema comum.)\n",
    "    \n",
    "Praticamente todo mundo que você conhece usa o Google. Uns ou outros utilizam o Bing e algumas pessoas perdidas no\n",
    "tempo usam o Yahoo!. Mas, algo razoável de se perguntar é: sempre foi assim? Provavelmente a resposta a essa\n",
    "pergunta é: Nem sempre. [posso reler a introdução do livro para ter ideias boas do que adicionar aqui.] | Mas\n",
    "então, **por que** o Google é _o Google_? A resposta disso em uma palavra é: PageRank.\n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "### PageRank\n",
    "    \n",
    "PageRank é um algoritmo criado por Sergey Brin e Larry Page, os fundadores da Google, no final de década de 90, que\n",
    "utiliza a estrutura de _links_ da Internet para dar uma classificação a suas páginas.\n",
    "\n",
    "A ideia central do algoritmo é resumida na seguinte frase: uma página é importante se páginas importantes levam a\n",
    "ela.  Em minha opinião, isso parece um pouco recursivo demais. Uma página será importante se outras páginas que\n",
    "ligam a ela forem importantes. Agora, essas páginas são importantes porque outras páginas importantes ligam a elas.\n",
    "Seguindo nesse raciocínio, chegamos na seguinte pergunta: quem são \"as primeiras\" páginas importantes? Essa\n",
    "recursão, na verdade, é algo embutido no algoritmo, algo que é trabalhado para se calcular a classificação de cada\n",
    "página. Conforme o texto, mais especificamente na seção de matemática, for progredindo, essa ideia ficará mais\n",
    "clara e veremos que, na verdade, não precisam existir \"as primeiras\" páginas importantes.\n",
    "\n",
    "Um fato curioso sobre o PageRank é que ele utiliza, praticamente, somente conceitos básicos da Álgebra Linear. Sim,\n",
    "um dos grandes motivos do porquê a _Google_ ser tão poderosa no mercado tecnológico de hoje em dia é devido, no seu\n",
    "início como empresa, à implementação de Álgebra Linear básica em algoritmos. Qualquer um que já fez algum curso de\n",
    "Álgebra Linear possui as ferramentas básicas para entender _como_ e o _por que_ o PageRank funciona. Esse é justo o\n",
    "foco central deste texto.\n",
    "    \n",
    "    \n",
    "    \n",
    "## A Matemática por trás\n",
    "\n",
    "[acho que é bom que a primeira seção do texto fale sobre Markov, assim terá nenhum problema em usar as suas\n",
    "terminologias, como por exemplo \"matriz estocástica\"]\n",
    "\n",
    "### A Matriz G\n",
    "\n",
    "Para começarmos, suponha um conjunto $P$ com $n$ páginas da Internet dadas por $P_i \\ (i = 1,2,\\cdots,n)$. Suponha\n",
    "também que as páginas desse conjunto possuem _links_ que vão para páginas do mesmo conjunto. Uma forma interessante\n",
    "de visualizar as páginas de $P$ e as ligações entre elas é por meio de um grafo. Por motivos didáticos, usaremos um\n",
    "conjunto $P$ com $n = 7$ páginas dadas pelo grafo abaixo.\n",
    "![Grafo](https://raw.githubusercontent.com/rangelalbuq/PageRank/main/Imagens/Grafo_1.png) Na imagem, os nós\n",
    "(círculos) representam as páginas e as arestas (setas) representam as ligações entre as páginas.\n",
    "\n",
    "Podemos representar esse grafo em um formato matricial. Seja $A$ uma matriz tal que $A_{ij}$ = 1, se a página $i$\n",
    "possuí um link para a página $j$, e $A_{ij}$ = 0 caso contrário (a página i não possui um _link_ para a página j).\n",
    "Acabamos de criar uma chamada matriz de _Adjacência_ do grafo. Assim, a matriz $A$ do grafo do conjunto de páginas\n",
    "$P$ é dada por\n",
    "\n",
    "$$ A = \\begin{bmatrix} 0&1&0&0&0&0&0 \\\\\n",
    "            0&0&1&0&0&0&0\\\\\n",
    "            1&0&0&1&0&0&1\\\\\n",
    "            0&0&0&0&1&0&0\\\\\n",
    "            0&0&0&0&0&1&0\\\\\n",
    "            0&0&0&1&0&0&0\\\\\n",
    "            0&0&0&0&0&0&0\n",
    "\\end{bmatrix} . $$\n",
    "\n",
    "Agora, vamos olhar para a matriz $A$ de uma forma diferente. E se o elemento $A_{ij}$ representasse a probabilidade\n",
    "de um usuário da internet ir à página $j$ considerando o fato dele estar, agora no momento, na página $i$?\n",
    "Observando $A$, vemos que essa interpretação nova não está consoante com ela e que um problema já visível está em\n",
    "sua linha 3. Segundo nossa interpretação, se um usuário estiver na página 3, a probabilidade dele ir para página 1\n",
    "é igual à 1. Porém, a chance dele ir para as páginas 4 e 7 também é 1, algo que não faz sentido. Um modo de\n",
    "contornar esse problema é criar uma _nova_ matriz que tenha a mesma “cara” de  $A$ e que também, corresponda com\n",
    "essa nova interpretação probabilística.\n",
    "\n",
    "Um modo de criar essa nova matriz, digamos $H$, de forma que, as probabilidades sejam “justas” ou “sem viés” é pela\n",
    "seguinte definição: o elemento $H_{ij}$ é igual à $(\\sum_{k=1}^{n}H_{ik})^{-1}$ se a página $i$ possuí um _link_\n",
    "para a página $j$ e $H_{ij} = 0$ caso contrário. Embora pareça um pouco complicado essa nova definição, saiba que a\n",
    "única diferença entra ela e a de $A$ é que estamos “normalizando” as linhas não-nulas para que a soma entre seus\n",
    "elementos seja igual à 1 e assim, faça sentido pensar nela como probabilidade. Assim, $H$ será dada por,\n",
    "\n",
    "$$ H = \\begin{bmatrix} 0&1&0&0&0&0&0 \\\\\n",
    "            0&0&1&0&0&0&0\\\\\n",
    "            1/3&0&0&1/3&0&0&1/3\\\\\n",
    "            0&0&0&0&1&0&0\\\\\n",
    "            0&0&0&0&0&1&0\\\\\n",
    "            0&0&0&1&0&0&0\\\\\n",
    "            0&0&0&0&0&0&0\n",
    "\\end{bmatrix} . $$\n",
    "\n",
    "A matriz $H$ agora é uma _matriz subestocástica_. Porém, ainda na matriz $H$ temos um problema. E a linha 7? Ela,\n",
    "por sua vez, possuí uma linha completa de zeros, o que quer dizer pela nossa interpretação que, se um usuário\n",
    "estiver na página 7, a probabilidade dele ir para qualquer outra página (de $P$) é zero. O que intuitivamente quer\n",
    "dizer que ele ficará na página 7 **para sempre**. Obviamente, isso é algo que não queremos que aconteça com nosso\n",
    "modelo, que a página 7 seja um “buraco negro” para nossos usuários, em que, se eles chegarem lá, viverão para\n",
    "sempre.\n",
    "\n",
    "Faremos o seguinte para contornar esse fato: se uma linha contiver apenas zeros, ela será alterada de forma que,\n",
    "todos seus elementos sejam iguais à $\\frac{1}{n}$, em que $n$ é o número de páginas de $P$ (dimensão de $H$). O que\n",
    "isso interpretativamente faz é que caso um usuário chegue a uma página que não possua ligação alguma com outra, ele\n",
    "escolhe “aleatoriamente” (com a mesma probabilidade) uma página qualquer de $P$ para ir.\n",
    "\n",
    "Assim, a matriz $H$ “atualizada”, o qual chamaremos de $S$, é dada por\n",
    "\n",
    "$$ S = \\begin{bmatrix} 0&1&0&0&0&0&0 \\\\\n",
    "            0&0&1&0&0&0&0\\\\\n",
    "            1/3&0&0&1/3&0&0&1/3\\\\\n",
    "            0&0&0&0&1&0&0\\\\\n",
    "            0&0&0&0&0&1&0\\\\\n",
    "            0&0&0&1&0&0&0\\\\\n",
    "            1/7&1/7&1/7&1/7&1/7&1/7&1/7 \n",
    "\\end{bmatrix} . $$\n",
    "\n",
    "A matriz $S$ agora é uma _matriz estocástica_.\n",
    "\n",
    "Porém, _incrivelmente_, ainda há mais um problema (e o último) com nosso modelo! Se, você leitor, observasse a\n",
    "matriz $S$ por um tempo suficientemente grande, provavelmente iria notar a seguinte propriedade da matriz: Se um\n",
    "usuário estiver na página 4, ele irá para a página 5. Se estiver na página 5, ele irá para a página 6. E se estiver\n",
    "na página 6, ele irá para a página 4. E assim por diante, para sempre. Criando assim um _loop_ eterno da navegação\n",
    "do mesmo. E com isso, a partir do momento em que entra pela primeira vez em uma dessas páginas, as outras ($P_i$\n",
    "com $i= 1,2,3,7$) efetivamente “não existirão” mais em nosso modelo, assim, não será possível quantificar algum\n",
    "tipo de classificação para as mesmas, mas apenas para aquelas que estão no ciclo.\n",
    "\n",
    "Para que o fato discutido acima não ocorra, consideraremos mais uma, e última modificação no comportamento do nosso\n",
    "usuário. Agora, antes de simplesmente selecionar um _link_ na página que atualmente se encontra, o usuário terá uma\n",
    "probabilidade $1 - \\alpha$ (com $\\alpha \\in (0,1)$) de ir para uma página qualquer de $P$. Isso, além trazer\n",
    "propriedades que garantirão o exito de nosso modelo, algo que veremos posteriormente, ela também traz a ele um\n",
    "comportamento esperado de qualquer um que navega a Internet. É razoável esperar de uma pessoa que ela não somente\n",
    "vá sendo levada site a site seguindo apenas os _links_ da página em que se encontra. Ela também pode, por exemplo,\n",
    "entrar em algum site em que a aba está aberta em seu navegador, ou também, abrir um de seu histórico por livre e\n",
    "espontânea vontade.\n",
    "\n",
    "Seja o vetor $e \\in \\mathbb{R}^n$ um vetor coluna com todas entradas iguais a 1 \n",
    "$$ e = \n",
    "\\begin{bmatrix} \n",
    "                1 \\\\\n",
    "                1 \\\\\n",
    "                \\vdots \\\\\n",
    "                1\n",
    "\\end{bmatrix}\n",
    ". \n",
    "$$\n",
    "A nova matriz criada a partir de $S$ será a chamada _matriz Google $G$_ que é dada pela seguinte equação:\n",
    "\n",
    "$$ G = \\alpha S + (1 - \\alpha)1/nee^T . $$\n",
    "\n",
    "Em que $1/nee^T \\in \\mathbb{R}^{n \\times n}$ é uma matriz de “teleportação aleatória”, o qual todos seus elementos\n",
    "são iguais à $\\frac{1}{n}$. Em nosso exemplo, escolhendo $\\alpha = 0.85$, a matriz G é\n",
    "\n",
    "$$ G = 0.85 \\begin{bmatrix} 0&1&0&0&0&0&0 \\\\\n",
    "            0&0&1&0&0&0&0\\\\\n",
    "            1/3&0&0&1/3&0&0&1/3\\\\\n",
    "            0&0&0&0&1&0&0\\\\\n",
    "            0&0&0&0&0&1&0\\\\\n",
    "            0&0&0&1&0&0&0\\\\\n",
    "            1/7&1/7&1/7&1/7&1/7&1/7&1/7 \n",
    "\\end{bmatrix}\n",
    "+ 0.15 \\begin{bmatrix} 1/7&1/7&1/7&1/7&1/7&1/7&1/7\\\\\n",
    "            1/7&1/7&1/7&1/7&1/7&1/7&1/7\\\\\n",
    "            1/7&1/7&1/7&1/7&1/7&1/7&1/7\\\\\n",
    "            1/7&1/7&1/7&1/7&1/7&1/7&1/7\\\\\n",
    "            1/7&1/7&1/7&1/7&1/7&1/7&1/7\\\\\n",
    "            1/7&1/7&1/7&1/7&1/7&1/7&1/7\\\\\n",
    "            1/7&1/7&1/7&1/7&1/7&1/7&1/7\n",
    "\\end{bmatrix} . $$\n",
    "\n",
    "### Fórmula para o PageRank\n",
    "\n",
    "Seguindo a filosofia de que _uma página é importante se páginas importantes direcionam a ela_, vamos definir uma\n",
    "fórmula para calcular o $PageRank$ de uma página. Uma possível fórmula poderia simplesmente ser a soma dos\n",
    "PageRanks das outras páginas que apontam para ela. Assim sendo, $$ r(P_i) = \\sum_{P_j \\in B_{P_i}}{r(P_j)}, $$ em\n",
    "que $r(P_i)$ é o PageRank da página $i$, $r(P_j)$ o da página $j$ e $B_{P_i}$ é o conjunto das páginas que apontam\n",
    "para a página $i$. Como cada $r(P_i)$ é um _rank_, temos que $r(P_i) > 0$. Para que o valor de algum $r(P_i)$ não\n",
    "\"exploda\", será também imposto a condição de que $\\sum_{i=1}^n r(P_i) = 1$.\n",
    "\n",
    "Porém, se questione do seguinte: imagine duas páginas os quais possuem o mesmo PageRank. Uma dessas páginas possui\n",
    "apenas um link para uma página qualquer, enquanto a outra possui links para cem páginas. O peso do link da primeira\n",
    "página deve ser o mesmo que o peso de cada link da segunda?\n",
    "\n",
    "Para os criadores do Google, a resposta é não. Quanto mais links uma página possui, a \"importância\" de cada um\n",
    "desses links deve valer menos. Portanto, para a fórmula do PageRank, é preciso ter um fator de peso que mede o quão\n",
    "\"expressivo\" é um link.\n",
    "\n",
    "Assim, a fórmula inicialmente usada por Sergey Brin e Larry Page é \n",
    "$$\n",
    "r(P_i) = \\sum_{P_j \\in B_{P_i}}\\frac{r(P_j)}{|P_j|}, \n",
    "$$\n",
    "em que $|P_j|$ é o número de links de $P_j$.\n",
    "\n",
    "O problema óbvio com essa fórmula é que simplesmente não sabemos os valores de $r(P_j)$. A forma utilizada para\n",
    "lidar com isso será aplicar a fórmula sucessivamente para as páginas de $P$, utilizando, a cada nova iteração, os\n",
    "valores obtidos para $r(P_j)$ da iteração prévia e torcer para que os valores de $r(P_j)$ convirjam para algo\n",
    "_estável_.  Assim, introduzindo uma nova notação, temos\n",
    "\n",
    "$$ r_{k+1}(P_i) = \\sum_{P_j \\in B_{P_i}}\\frac{r_k(P_j)}{|P_j|}. $$\n",
    "\n",
    "Para que a fórmula acima faça sentido, é preciso ainda saber quem são os $r_0(P_i)$, $i = 1,\\dots, n$, que é justo\n",
    "nossa dúvida inicial discutida na seção \"PageRank\". Como inicialmente conhecemos nada sobre as páginas $P_i$, é\n",
    "razoável pensar que todas, no começo do processo, valem o mesmo. Portanto a condição inicial dos $r_k(P_i)$ será\n",
    "$r_0(P_i) = \\frac{1}{n}$ para $i = 1,\\dots, n$.\n",
    "\n",
    "Agora que a mágica começa a acontecer. Pela equação acima, temos um sistema de $n$ equações em que cada uma depende\n",
    "da outra. Em todas, os $r_k(P_i)$ são \"pesados\" por diferentes coeficientes em cada equação, contando também com a\n",
    "\"pesagem\" por $0$. E ainda por cima, há um somatório em todas equações. Isso lembra uma multiplicação matriz-vetor.\n",
    "E de fato, esse sistema realmente pode ser representado desta forma.\n",
    "\n",
    "Os $r_k(P_i)$, $i = 1, \\dots, n$ serão as componentes de um vetor $\\pi^k$ e os \"pesos\" $|P_i|$, $i = 1,\\dots, n$\n",
    "farão\n",
    "parte da matriz de coeficientes.\n",
    "\n",
    "Visto que há condições impostas aos todos $r_k(P_i)$ $i=1,\\dots,n$, $\\pi^k$ também as terá. Primeiramete, temos que\n",
    "$r_k(P_i) = \\pi_i^k > 0$ o que implica em $\\pi^k > 0$. E também que como $\\sum_{i=1}^n r_k(P_i) = \\sum_{i=1}^n\n",
    "|r_k(P_i)| = 1$ isso significa que $\\|\\pi^k\\|_1 = 1$.\n",
    "\n",
    "Agora você leitor, consegue lembrar de alguma matriz que era composta por \"pesos\", que juntos somavam a $1$ e que\n",
    "dependiam somente do quanto de _links_ uma página $P_i$ possuía para outras páginas? Essa é justo a matriz $H$.\n",
    "Para lembrança, a matriz $H$ do exemplo é\n",
    "\n",
    "$$ \n",
    "H = \n",
    "\\begin{bmatrix}\n",
    "            0&1&0&0&0&0&0 \\\\\n",
    "            0&0&1&0&0&0&0\\\\\n",
    "            1/3&0&0&1/3&0&0&1/3\\\\\n",
    "            0&0&0&0&1&0&0\\\\\n",
    "            0&0&0&0&0&1&0\\\\\n",
    "            0&0&0&1&0&0&0\\\\\n",
    "            0&0&0&0&0&0&0\n",
    "\\end{bmatrix}\n",
    ". \n",
    "$$\n",
    "\n",
    "O que nos falta agora é se decidir se a multiplicação será dada pela direita, $\\pi^{k+1} = H\\pi^k$, ou pela\n",
    "esquerda, $(\\pi^t)^{k+1} = (\\pi^t)^kH$. Como os elementos de uma coluna $i$ de $H$ representam as probabilidades do\n",
    "usuário na página da linha $j$ ir a página $i$, esses são justos os \"pesos\" da fórmula. Assim, a multiplicação\n",
    "matriz-vetor deve ser feito pela esquerda.\n",
    "\n",
    "$$\n",
    "(\\pi^t)^{k+1}\n",
    "=\n",
    "(\\pi^t)^kH . \n",
    "$$\n",
    "\n",
    "Porém, como já foi discutido na seção \"A Matriz G\", sabemos que a utilização da matriz $H$ trás problemas para o\n",
    "modelo. Portanto, em vez de $\\pi^k$ ser multiplicado por $H$, ele será multiplicado por $G$. Desse modo tem-se,\n",
    "\n",
    "$$\n",
    "(\\pi^t)^{k+1}\n",
    "=\n",
    "(\\pi^t)^kG . \n",
    "$$\n",
    "\n",
    "Uma implicação \"ruim\" do uso da $G$ na equação é que ela não nos dará o \"real\" ranqueamento de cada página. Isso se\n",
    "deve ao fato de $G$ ser o resultado de uma soma entre $S$ e $ee^t$. A matriz $S$ preserva a estrutura de links das\n",
    "páginas da Internet e lida de forma \"especial\" com páginas sem links. Enquanto $ee^t$, que é uma matriz de\n",
    "teleportação aleatória, não preserva a estrutura. Ainda assim é necessário a utilização de $G$ para, pelo menos,\n",
    "obter uma aproximação do _rank_ \"real\" das páginas. Não é possível de se calcular este _rank_ \"real\", utilizando\n",
    "somente $S$. Apenas o uso dela pode acarretar no fenômeno do usuário fictício seguir um caminho periódico\n",
    "previsível entre as páginas, algo que foi visto na seção \"A matriz G\". Ou, de  forma mais geral, o usuário pode\n",
    "navegar somente num conglomerado de páginas sem ter a possibilidade do mesmo conseguir  \"chegar\" em alguma página\n",
    "fora desse conglomerado. Em qualquer um desses casos, o _PageRank_ das páginas que não pertencem a esse\n",
    "conglomerado iria tender a zero conforme o cálculo dos _ranks_ for sendo executado. O que implica na falha do\n",
    "modelo para o cálculo do _rank_ dessas páginas, algo não desejado.\n",
    "\n",
    "Esperamos que os elementos de $\\pi^k$, $\\pi_i^k = r_k(P_i)$, convirjam para um valor estável depois de várias\n",
    "iterações, isso significa que para um $k$ \"grande\", $\\pi^k \\approx \\pi$, para certo $\\pi$. Assim podemos escrever\n",
    "que para $k \\rightarrow \\infty$, $\\pi^k = \\pi$ em que $\\pi$ satisfaz \n",
    "$$\n",
    "\\pi^t\n",
    "=\n",
    "\\pi^tG. \n",
    "$$\n",
    " \n",
    "Deste modo, o problema do _PageRank_ se reduz a encontrar um vetor $\\pi$ que satisfaz as condições $\\pi > 0$,\n",
    "$\\|\\pi\\|_1$ = 1 e também $\\pi^t = \\pi^tG$.\n",
    "\n",
    "[Aqui seria bom discutir o fato de não sabermos se o vetor $\\pi$ existe, se ele é único e também se o método\n",
    "proposto para calculá-lo \"funciona\", se ele converje ou não.]\n",
    "\n",
    "### Autovalor e Autovetor\n",
    "\n",
    "Agora antes de continuar, vamos fazer uma pequena pausa no racicíonio para relembrar o conceito de autovalor e\n",
    "autovetor que será importante para o entendimento deste texto.\n",
    "\n",
    "Você provavelmente está acostumado com a seguinte definição de autovalor e autovetor: dada uma matriz qualquer $A\n",
    "\\in \\mathbb{R}^{n \\times n}$ e um vetor $v \\in \\mathbb{R}^n$, com $v \\neq 0$, $v$ é autovetor de $A$ associado ao\n",
    "autovalor $\\lambda$ se $$ Av = \\lambda v,$$ para algum $\\lambda \\in \\mathbb{R}$. Uma forma de se calcular os\n",
    "autovalores $\\lambda$ é pelo _polinômio característico_ de $A$, $p(A) = det(A-\\lambda I)$.\n",
    "\n",
    "Porém, mesmo A possuindo apenas entradas reais, as raízes de $p(A)$ podem assumir valores complexos. Assim, se nos\n",
    "restringirmos a valores reais para os autovalores $\\lambda$, é possível que haja a \"perda\" de certos $\\lambda$.\n",
    "Isso implicaria que a \"quantidade\", multiplicidade algébrica, de autovalores de $A$ seja menor do que $n$, sua\n",
    "dimensão, o que implicaria em problemas para nossa análise. Portanto, para que nenhum $\\lambda$ fique de fora, será\n",
    "adotada a seguinte definição de autovalor e autovetor:\n",
    "\n",
    "Dada uma matriz $A \\in \\mathbb{C}^{n \\times n}$ e um vetor $v \\in \\mathbb{C}^n$, com $v \\neq 0$, $v$ é autovetor de\n",
    "$A$ associado ao autovalor $\\lambda$ se $$ Av = \\lambda v,$$ para algum $\\lambda \\in \\mathbb{C}$.\n",
    "\n",
    "O conjunto de autovalores $\\lambda$ que uma matriz $A$ possui será chamado de _espectro_ de A, denotado por\n",
    "$\\sigma(A)$.  O valor absoluto dos maiores $\\lambda$ em módulo, $|\\lambda _i| \\geq |\\lambda _j|$, será chamado de\n",
    "_raio espectral_, denotado por $\\rho(A)$. \n",
    "\n",
    "Uma observação importante é que existem tanto autovetor à direita, $Ax = \\lambda x$, quanto à esquerda, $y^tA =\n",
    "\\lambda y^t$. Os autovetores à direita são os que estamos mais habituados a lidar. Mas não se preocupe, se você\n",
    "entende autovetores à direita você  também entende o à esquerda. É praticamente a mesma ideia. Algo não difícil de\n",
    "provar é que o espectro $\\sigma(A)$ de uma matriz A ,o conjunto dos autovalores de uma matriz, dos autovetores à\n",
    "direita e dos à esquerda de $A$ é o mesmo (tente provar você mesmo!), assim, se existem autovetores à direita para\n",
    "um certo $\\lambda$ irá também existir autovetores à esquerda para o mesmo $\\lambda$. Porém há sim, certas\n",
    "diferenças entre ambos, como por exemplo que se um vetor $v$ é autovetor à direita associado a um autovalor\n",
    "$\\lambda$ isso não implica que $v^t$ também será autovetor à esquerda associado a $\\lambda$.\n",
    "\n",
    "Dito isso e olhando a equação que chegamos na seção anterior, $\\pi^t = \\pi^tG$, sabemos agora que o problema do\n",
    "_PageRank_ é também um problema de autovalor e autovetor. No caso, queremos encontrar o autovetor à esquerda de\n",
    "$G$, que satisfaz certas condições, associado ao autovalor 1.\n",
    "\n",
    "Como tanto $\\pi$, $\\lambda = 1$ e $G$ pertencem aos reais, temos que todas as iterações $\\pi^k$ pertencerão aos\n",
    "reais.  Assim, mesmo aparecendo números complexos na nossa nova definição de autovalor e autovetor, só utilizaremos\n",
    "números reais para chegar ao vetor $\\pi$. Portanto não precisamos nos preocupar com a aritmética complexa no\n",
    "problema do _PageRank_.\n",
    "\n",
    "Agora que abordamos o conceito de autovalores e autovetores, podemos começar a encontrar respostas para os\n",
    "questionamentos a respeito do vetor $\\pi$. Primeiramente, para discutir sobre a existência e unicidade,\n",
    "utilizaremos um teorema importante na área de Álgebra Linear o chamado teorema de Perron.\n",
    "\n",
    "\n",
    "### Perron\n",
    "\n",
    "O teorema de Perron infere propriedades de um autovalor específico de uma matriz $A$ que satisfaz apenas duas\n",
    "condições: A é quadrada, $A \\in \\mathbb{R}^{n \\times n}$, e que todos seus elementos sejam positivos, $a_{ij} > 0,\n",
    "i,j = 1,2,\\dots,n$.\n",
    "\n",
    "Dentre todas as afirmações que o teorema diz, as mais importantes para nós são as seguintes: Seja A uma matriz com\n",
    "entradas positivas. Existe um único autovetor $p$ à direita de $A$, com $p > 0$ e $\\|p\\|_1 = 1$, chamado _vetor de\n",
    "Perron_, associado a um autovalor $\\lambda = r > 0$, chamado de _raiz de Perron_, com multiplicidade algébrica\n",
    "igual à 1. Além de $p$ ser único, ele ainda possui a \"unicidade\" de, exceto de seus múltiplos positivos, não haver\n",
    "outros autovetores positivos à direita de $A$, independentemente do autovalor. E, além de tudo isso, $r$ é o maior\n",
    "autovalor em magnitude de $A$, com $|r| = \\rho(A)$. \n",
    "\n",
    "No teorema está sendo apenas descrito o _autovetor de Perron_ à direita tal que $Ap = rp$. Porém ainda assim existe\n",
    "o _autovetor de Perron_ à esquerda, digamos $q$, com as mesmas condições e propriedades de $p$ associado também a\n",
    "_raiz de Perron_ $r$ em que $q^tA = rq^t$.\n",
    "\n",
    "\n",
    "A _matriz Google_ $G$ satisfaz as duas condições iniciais do teorema, assim deve existir um autovalor $\\lambda = r$\n",
    "de $G$ que possui as propriedades descritas acima. O que nos resta agora é encontrá-lo.\n",
    "\n",
    "Note que a multiplicação entre $G$ e $e$, o vetor coluna com todas entradas iguais a 1, representa a soma dos\n",
    "elementos das linhas de $G$, que por construção é igual à 1. Deste modo temos que,  \n",
    "$$ Ge = e. $$\n",
    "\n",
    "Esse resultado implica que o vetor $e$ é um autovetor à direita de $G$ com autovalor $\\lambda = 1$. sabemos pelo\n",
    "Teorema de Perron que autovetores positivos devem ter como autovalor associado $\\lambda = r$. Como $e> 0$ e tem\n",
    "como autovalor associado $\\lambda = 1$, concluímos que $r = 1$.\n",
    "\n",
    "Agora que sabemos que o _autovalor de Perron_ de $G$ é 1, podemos nos apropriar das propriedades do teorema. \n",
    "\n",
    "[Cadeia de Markov $\\rightarrow \\dots \\rightarrow$ Autovalor e Autovetor $\\rightarrow$ Perron $\\rightarrow$ Método\n",
    "da Potência $\\rightarrow$ ...\n",
    "\n",
    "Perron: Teorema. Autovalor de Perron é 1 e $\\pi$ é o autovetor de Perron à esquerda de G. (nota: não é preciso\n",
    "falar de Perron-Frobenius, mas sim apenas de Perron. A matriz G é > 0 implicando que o Teorema de Perron já basta\n",
    "para responder sobre a existência, unicidade e que $\\lambda = 1$ é o único autovalor em $\\rho(A)$.)\n",
    "\n",
    "Método da Potência: Teoria. 1 é o maior e $\\alpha$ é o segundo maior autovalor em magnitude de G. $\\pi$ é\n",
    "calculável.  ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6dcdfd1c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "Cell": {
   "cm_config": {
    "lineWrapping": true
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
