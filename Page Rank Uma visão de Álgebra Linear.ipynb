{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "913ea05e",
   "metadata": {},
   "source": [
    "# Introdução\n",
    "\n",
    "(Muitos dos quais lerão esse texto, raramente em sua vida, se depararam com o problema de ter que utilizar outro\n",
    "site de busca, além do Google, para encontrar algo que procura na internet. Porém, quando ela ainda estava em sua\n",
    "fase “jovem”, isso era um problema comum.)\n",
    "    \n",
    "Praticamente todo mundo que você conhece usa o Google. Uns ou outros utilizam o Bing e algumas pessoas perdidas no\n",
    "tempo usam o Yahoo!. Mas, algo razoável de se perguntar é: sempre foi assim? Provavelmente a resposta a essa\n",
    "pergunta é: Nem sempre. [posso reler a introdução do livro para ter ideias boas do que adicionar aqui.] | Mas\n",
    "então, **por que** o Google é _o Google_? A resposta disso em uma palavra é: PageRank.\n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "## PageRank\n",
    "    \n",
    "PageRank é um algoritmo criado por Sergey Brin e Larry Page, os fundadores da Google, no final de década de 90, que\n",
    "utiliza a estrutura de _links_ da Internet para dar uma classificação a suas páginas.\n",
    "\n",
    "A ideia central do algoritmo é resumida na seguinte frase: uma página é importante se páginas importantes levam a\n",
    "ela.  Em minha opinião, isso parece um pouco recursivo demais. Uma página será importante se outras páginas que\n",
    "ligam a ela forem importantes. Agora, essas páginas são importantes porque outras páginas importantes ligam a elas.\n",
    "Seguindo nesse raciocínio, chegamos na seguinte pergunta: quem são \"as primeiras\" páginas importantes? Essa\n",
    "recursão, na verdade, é algo embutido no algoritmo, algo que é trabalhado para se calcular a classificação de cada\n",
    "página. Conforme o texto, mais especificamente na seção de matemática, for progredindo, essa ideia ficará mais\n",
    "clara e veremos que, na verdade, não precisam existir \"as primeiras\" páginas importantes.\n",
    "\n",
    "Um fato curioso sobre o PageRank é que ele utiliza, praticamente, somente conceitos básicos da Álgebra Linear. Sim,\n",
    "um dos grandes motivos do porquê a _Google_ ser tão poderosa no mercado tecnológico de hoje em dia é devido, no seu\n",
    "início como empresa, à implementação de Álgebra Linear básica em algoritmos. Qualquer um que já fez algum curso de\n",
    "Álgebra Linear possui as ferramentas básicas para entender _como_ e o _por que_ o PageRank funciona. Esse é justo o\n",
    "foco central deste texto.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6dcdfd1c",
   "metadata": {},
   "source": [
    "# A Matemática por trás\n",
    "\n",
    "## Cadeias de Markov\n",
    "\n",
    "Inicialmente, discutiremos a respeito de cadeias de Markov. Elas o ajudarão no entendimento e a ligar conceitos ao\n",
    "longo do texto. Você provavelmente não conhece nada a respeito do que é uma cadeia de Markov. Portanto, aqui só\n",
    "introduziremos seus conceitos fundamentais.\n",
    "\n",
    "É verdade que, conforme comentado acima, apenas o conhecimento básico de Álgebra Linear já é suficiente para o\n",
    "entendimento de como funciona o PageRank. Porém, conhecer os fundamentos de cadeias de Markov o ajuda a entender\n",
    "uma área da matemática, provavelmente nova para você, leitor, e ter uma melhor noção \"do que está acontecendo\". E,\n",
    "além do mais, não machuca ninguém.\n",
    "\n",
    "Primeiramente, um _processo estocástico_ é um conjunto de variáveis aleatórias $\\left\\{X_t \\right\\}_{t=0}^{\\infty}$\n",
    "que possuem um conjunto de possíveis valores em comum $S = \\left\\{S_1, S_2, \\dots, S_n\\right\\}$, que é chamado de\n",
    "_espaço de estados_ para o processo. O parâmetro $t$ é geralmente pensado como tempo, e $X_t$ representa o estado\n",
    "do processo no tempo $t$.  Por exemplo, considere o processo de navegar na Internet clicando em links que vão de\n",
    "uma página a outra. O espaço de estados $S$ é o conjunto de todas a páginas da Internet e a variável aleatória $X_t$\n",
    "é a página visitada no tempo $t$.\n",
    "\n",
    "Uma _cadeia de Markov_ é um processo estocástico com a propriedade de ser um processo \"sem memória\". A\n",
    "probabilidade da cadeia ir para algum estado depende somente do estado atual que ela se encontra e não dos\n",
    "estados anteriores. Escrevendo isso em forma de equação temos,\n",
    "$$\n",
    "P\\left(X_{t+1}=S_j | X_t=S_{i_t},X_{t-1}=S_{i_{i-1}},\\dots,X_0=S_{i_0}\\right) = P\\left(X_{t+1}=S_j | X_t=S_{i_t}\\right)\n",
    "$$\n",
    "para cada $t = 0, 1, 2, \\dots$. A notação $P\\left(E | F\\right)$ representa a probabilidade condicional do evento\n",
    "$E$ ocorrer dado que $F$ ocorreu. Por exemplo, é uma cadeia de Markov o processo de um usuário navegar na Internet\n",
    "selecionando links que pertencem a página atual em que o usuário se encontra.\n",
    "\n",
    "A _probabilidade de transição_ $p_{ij} = P\\left(X_t=S_j | X_{t-1}=S_i\\right)$ é a probabilidade de estar no estado\n",
    "$S_j$ no tempo $t$ com o fato da cadeia ter passado no estado $S_i$ em $t-1$. Assim, é basicamente a probabilidade\n",
    "de ir para $S_j$ de $S_i$ no tempo $t$.\n",
    "\n",
    "A _matriz de probabilidade de transição_ é definida como sendo uma matriz em que seus elementos são probabilidades\n",
    "de transição, $P_{n \\times n}(t) = \\left[p_{ij}(t)\\right]$. Ela é não-negativa, visto que seus elementos são todos\n",
    "probabilidades. Nota-se também que a soma dos elementos de cada linha deve ser 1, já que a soma das probabilidades\n",
    "de ir para qualquer estado pertencente ao espaço de estados $S$ deve-se somar a 1. Matrizes que satisfazem essas\n",
    "propriedades de serem não-negativas e que a soma dos elementos de cada linha ser igual a 1 são chamadas de\n",
    "_matrizes estocásticas_. Portanto, dado um $t$ específico, $P(t)$ é uma matriz estocástica.  \n",
    "\n",
    "Uma _cadeia de Markov_ é _estacionária_ se as probabilidades de transição não variarem com o tempo, isto é $p_{ij}(t) =\n",
    "p_{ij}$ para todo $t$.\n",
    "\n",
    "Um _vetor de distribuição de probabilidade_ (ou apenas \"vetor de probabilidade\") é definido com sendo um vetor $p^t\n",
    "= (p_1, p_2, \\dots, p_n)$ não-negativo de tal forma que a soma de seus elementos é igual a 1. \n",
    "\n",
    "Um _vetor de distribuição de probabilidade estacionário_ para uma cadeia de Markov estacionária que tem $P$ como matriz de\n",
    "transição é um vetor $\\pi$ de tal forma que $\\pi^t = \\pi^t P$.\n",
    "\n",
    "Agora que conhecemos alguns novos conceitos de cadeias de Markov, vamos abordar o problema geral do PageRank."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4d9acbf",
   "metadata": {},
   "source": [
    "## A Matriz Google\n",
    "\n",
    "Para começarmos, suponha um conjunto $P$ com $n$ páginas da Internet dadas por $P_i \\ (i = 1,2,\\cdots,n)$. Suponha\n",
    "também que as páginas desse conjunto possuem _links_ que vão para páginas do mesmo conjunto. Uma forma interessante\n",
    "de visualizar as páginas de $P$ e as ligações entre elas é por meio de um grafo. Por motivos didáticos, usaremos um\n",
    "conjunto $P$ com $n = 7$ páginas dadas pelo grafo abaixo.\n",
    "\n",
    "![Grafo](Imagens/Grafo_1.png) \n",
    "\n",
    "Na imagem, os nós\n",
    "(círculos) representam as páginas e as arestas (setas) representam as ligações entre as páginas.\n",
    "\n",
    "Podemos representar esse grafo em um formato matricial. Seja $A$ uma matriz tal que $a_{ij}$ = 1, se a página $i$\n",
    "possuí um link para a página $j$, e $a_{ij}$ = 0 caso contrário (a página $i$ não possui um _link_ para a página\n",
    "$j$).  Acabamos de criar uma chamada matriz de _Adjacência_ do grafo. Assim, a matriz $A$ do grafo do conjunto de\n",
    "páginas $P$ é dada por\n",
    "\n",
    "$$ A = \\begin{bmatrix} \n",
    "            0&1&0&0&0&0&0\\\\\n",
    "            0&0&1&0&0&0&0\\\\\n",
    "            1&0&0&1&0&0&1\\\\\n",
    "            0&0&0&0&1&0&0\\\\\n",
    "            0&0&0&0&0&1&0\\\\\n",
    "            0&0&0&1&0&0&0\\\\\n",
    "            0&0&0&0&0&0&0\n",
    "\\end{bmatrix} . $$\n",
    "\n",
    "Agora, vamos olhar para a matriz $A$ de uma forma diferente. E se o elemento $a_{ij}$ representasse a probabilidade\n",
    "de um usuário da Internet ir à página $j$ considerando o fato dele estar, agora no momento, na página $i$?\n",
    "Observando $A$, vemos que essa interpretação nova não está consoante com a matriz e que um problema já visível está\n",
    "em sua linha 3. Segundo nossa interpretação, se um usuário estiver na página 3, a probabilidade dele ir para página\n",
    "1 é igual à 1. Porém, a chance dele ir para as páginas 4 e 7 também é 1, algo que não faz sentido. Um modo de\n",
    "contornar esse problema é criar uma _nova_ matriz que tenha a mesma “cara” de  $A$ e que também, corresponda com\n",
    "essa nova interpretação probabilística.\n",
    "\n",
    "Um modo de criar essa nova matriz, digamos $H$, de forma que, as probabilidades sejam “justas” ou “sem viés” é pela\n",
    "seguinte definição: o elemento $h_{ij}$ é igual à $(\\sum_{k=1}^{n}h_{ik})^{-1}$ se a página $i$ possuí um _link_\n",
    "para a página $j$ e $h_{ij} = 0$ caso contrário. Embora pareça um pouco complicado essa nova definição, saiba que a\n",
    "única diferença entra ela e a de $A$ é que estamos “normalizando” as linhas não-nulas para que a soma entre seus\n",
    "elementos seja igual à 1 e assim, faça sentido pensar nela como probabilidade. Assim, $H$ será dada por,\n",
    "\n",
    "$$ H = \\begin{bmatrix} 0&1&0&0&0&0&0 \\\\\n",
    "            0&0&1&0&0&0&0\\\\\n",
    "            1/3&0&0&1/3&0&0&1/3\\\\\n",
    "            0&0&0&0&1&0&0\\\\\n",
    "            0&0&0&0&0&1&0\\\\\n",
    "            0&0&0&1&0&0&0\\\\\n",
    "            0&0&0&0&0&0&0\n",
    "\\end{bmatrix} . $$\n",
    "\n",
    "A matriz $H$ é uma chamada _matriz subestocástica_, visto que todas suas linhas não-nulas somam a 1. Porém, ainda\n",
    "na matriz $H$ temos um problema. E a linha 7? Ela, por sua vez, possuí uma linha completa de zeros, o que quer\n",
    "dizer pela nossa interpretação que, se um usuário estiver na página 7, a probabilidade dele ir para qualquer outra\n",
    "página (de $P$) é zero. O que intuitivamente quer dizer que ele ficará na página 7 **para sempre**.  Obviamente,\n",
    "isso é algo que não queremos que aconteça com nosso modelo, que a página 7 seja um “buraco negro” para nossos\n",
    "usuários, em que, se eles chegarem lá, viverão para sempre.\n",
    "\n",
    "Faremos o seguinte para contornar esse fato: se uma linha contiver apenas zeros, ela será alterada de forma que,\n",
    "todos seus elementos sejam iguais à $\\frac{1}{n}$, em que $n$ é o número de páginas de $P$ (dimensão de $H$). O que\n",
    "isso interpretativamente faz é que caso um usuário chegue a uma página que não possua ligação alguma com outra, ele\n",
    "se direcionará a alguma página de $P$. É como se ele escolhesse uma página do seu histórico de navegação\n",
    "“aleatoriamente”.\n",
    "\n",
    "Assim, a matriz $H$ “atualizada”, o qual chamaremos de $S$, é dada por\n",
    "\n",
    "$$ S = \\begin{bmatrix} 0&1&0&0&0&0&0 \\\\\n",
    "            0&0&1&0&0&0&0\\\\\n",
    "            1/3&0&0&1/3&0&0&1/3\\\\\n",
    "            0&0&0&0&1&0&0\\\\\n",
    "            0&0&0&0&0&1&0\\\\\n",
    "            0&0&0&1&0&0&0\\\\\n",
    "            1/7&1/7&1/7&1/7&1/7&1/7&1/7 \n",
    "\\end{bmatrix} . $$\n",
    "\n",
    "Recorde que cada elemento de $S$, $s_{ij}$, representa a probabilidade de um usuário ir de uma página $i$ para uma\n",
    "página $j$, assim $s_{ij} = P\\left(X_{t+1} = S_j | X_t = S_i\\right)$, com $X_t$ sendo a página visitada no tempo\n",
    "$t$, $S_j$ e $S_i$ sendo, respectivamente, a página $j$ e a página $i$. Portanto, devido a nossa construção, $S$ é\n",
    "uma _matriz de probabilidade de transição estocástica_ com cada elemento seu $s_{ij}$ sendo uma _probabilidade de\n",
    "transição_ entre páginas.\n",
    "\n",
    "Porém, _incrivelmente_, ainda há mais um problema (e o último) com nosso modelo. Se, você leitor, observasse a\n",
    "matriz $S$ por um tempo suficientemente grande, provavelmente iria notar a seguinte propriedade da matriz: Se um\n",
    "usuário estiver na página 4, ele irá para a página 5. Se estiver na página 5, ele irá para a página 6. E se estiver\n",
    "na página 6, ele irá para a página 4. E assim por diante, para sempre. Criando assim um _loop_ eterno da navegação\n",
    "do mesmo. E com isso, a partir do momento em que entra pela primeira vez em uma dessas páginas, as outras ($P_i$\n",
    "com $i= 1,2,3,7$) efetivamente “não existirão” mais em nosso modelo, dessa maneira, não será possível quantificar\n",
    "algum tipo de classificação numérica para as mesmas, mas apenas para aquelas que estão no ciclo.\n",
    "\n",
    "Para que o fato discutido acima não ocorra, consideraremos mais uma, e última modificação no comportamento do nosso\n",
    "usuário. Agora, antes de simplesmente selecionar um _link_ na página que atualmente se encontra, o usuário terá uma\n",
    "probabilidade $1 - \\alpha$ (com $\\alpha \\in (0,1)$) de ir para uma página qualquer de $P$. Isso, além trazer\n",
    "propriedades que garantirão o êxito de nosso modelo, algo que veremos posteriormente, ela também traz a ele um\n",
    "comportamento esperado de qualquer um que navega a Internet. É razoável esperar de uma pessoa que ela não somente\n",
    "vá sendo levada site a site seguindo apenas os _links_ da página em que se encontra. Ela também pode, por exemplo,\n",
    "entrar em algum site em que a aba está aberta em seu navegador, ou também, abrir um de seu histórico por livre e\n",
    "espontânea vontade.\n",
    "\n",
    "Seja o vetor $e \\in \\mathbb{R}^n$ um vetor coluna com todas entradas iguais a 1 \n",
    "$$ e = \n",
    "\\begin{bmatrix} \n",
    "                1 \\\\\n",
    "                1 \\\\\n",
    "                \\vdots \\\\\n",
    "                1\n",
    "\\end{bmatrix}\n",
    ". \n",
    "$$\n",
    "A nova matriz criada a partir de $S$ será a chamada _matriz Google_ $G$ que é dada pela seguinte equação:\n",
    "\n",
    "$$ G = \\alpha S + (1 - \\alpha)1/nee^T . $$\n",
    "\n",
    "Em que $1/nee^T \\in \\mathbb{R}^{n \\times n}$ é uma matriz de “teleportação aleatória”, o qual todos seus elementos\n",
    "são iguais à $\\frac{1}{n}$. Em nosso exemplo, escolhendo $\\alpha = 0.85$, a matriz G é\n",
    "\n",
    "$$ G = 0.85 \\begin{bmatrix} 0&1&0&0&0&0&0 \\\\\n",
    "            0&0&1&0&0&0&0\\\\\n",
    "            1/3&0&0&1/3&0&0&1/3\\\\\n",
    "            0&0&0&0&1&0&0\\\\\n",
    "            0&0&0&0&0&1&0\\\\\n",
    "            0&0&0&1&0&0&0\\\\\n",
    "            1/7&1/7&1/7&1/7&1/7&1/7&1/7 \n",
    "\\end{bmatrix}\n",
    "+ 0.15 \\begin{bmatrix} 1/7&1/7&1/7&1/7&1/7&1/7&1/7\\\\\n",
    "            1/7&1/7&1/7&1/7&1/7&1/7&1/7\\\\\n",
    "            1/7&1/7&1/7&1/7&1/7&1/7&1/7\\\\\n",
    "            1/7&1/7&1/7&1/7&1/7&1/7&1/7\\\\\n",
    "            1/7&1/7&1/7&1/7&1/7&1/7&1/7\\\\\n",
    "            1/7&1/7&1/7&1/7&1/7&1/7&1/7\\\\\n",
    "            1/7&1/7&1/7&1/7&1/7&1/7&1/7\n",
    "\\end{bmatrix} . $$\n",
    "\n",
    "Note que, que nem a matriz $S$, $G$ também é uma _matriz de probabilidade de transição estocástica_ com cada\n",
    "elemento seu $g_{ij}$ sendo uma _probabilidade de transição_ entre páginas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6796c0b",
   "metadata": {},
   "source": [
    "## O Vetor do PageRank\n",
    "\n",
    "Seguindo a filosofia de que _uma página é importante se páginas importantes direcionam a ela_, vamos definir uma\n",
    "fórmula para calcular o $PageRank$ de uma página. Uma possível fórmula poderia simplesmente ser a soma dos\n",
    "PageRanks das outras páginas que apontam para ela. Assim sendo, $$ r(P_i) = \\sum_{P_j \\in B_{P_i}}{r(P_j)}, $$ em\n",
    "que $r(P_i)$ é o PageRank da página $i$, $r(P_j)$ o da página $j$ e $B_{P_i}$ é o conjunto das páginas que apontam\n",
    "para a página $i$. Como cada $r(P_i)$ é um _rank_, temos que $r(P_i) > 0$. Para que o valor de algum $r(P_i)$ não\n",
    "\"exploda\", será também imposto a condição de que $\\sum_{i=1}^n r(P_i) = 1$.\n",
    "\n",
    "Porém, se questione do seguinte: imagine duas páginas os quais possuem o mesmo PageRank. Uma dessas páginas possui\n",
    "apenas um link para uma página qualquer, enquanto a outra possui links para cem páginas. O peso do link da primeira\n",
    "página deve ser o mesmo que o peso de cada link da segunda?\n",
    "\n",
    "Para os criadores do Google, a resposta é não. Quanto mais links uma página possui, a \"importância\" de cada um\n",
    "desses links deve valer menos. Portanto, para a fórmula do PageRank, é preciso ter um fator de peso que mede o quão\n",
    "\"expressivo\" é um link.\n",
    "\n",
    "Assim, a fórmula inicialmente usada por Sergey Brin e Larry Page é \n",
    "$$\n",
    "r(P_i) = \\sum_{P_j \\in B_{P_i}}\\frac{r(P_j)}{|P_j|}, \n",
    "$$\n",
    "em que $|P_j|$ é o número de links de $P_j$.\n",
    "\n",
    "O problema óbvio com essa fórmula é que simplesmente não sabemos os valores de $r(P_j)$. A forma utilizada para\n",
    "lidar com isso será aplicar a fórmula sucessivamente para as páginas de $P$, utilizando, a cada nova iteração, os\n",
    "valores obtidos para $r(P_j)$ da iteração prévia e torcer para que os valores de $r(P_j)$ convirjam para algo\n",
    "_estável_.  Assim, introduzindo uma nova notação, temos\n",
    "\n",
    "$$ r_{k+1}(P_i) = \\sum_{P_j \\in B_{P_i}}\\frac{r_k(P_j)}{|P_j|}. $$\n",
    "\n",
    "Para que a fórmula acima faça sentido, é preciso ainda saber quem são os $r_0(P_i)$, $i = 1,\\dots, n$, que é justo\n",
    "nossa dúvida inicial discutida na seção \"PageRank\". Como inicialmente conhecemos nada sobre as páginas $P_i$, é\n",
    "razoável pensar que todas, no começo do processo, valem o mesmo. Portanto a condição inicial dos $r_k(P_i)$ será\n",
    "$r_0(P_i) = \\frac{1}{n}$ para $i = 1,\\dots, n$.\n",
    "\n",
    "Agora que a mágica começa a acontecer. Pela equação acima, temos um sistema de $n$ equações em que cada uma depende\n",
    "da outra. Em todas, os $r_k(P_i)$ são \"pesados\" por diferentes coeficientes em cada equação, contando também com a\n",
    "\"pesagem\" por $0$. E ainda por cima, há um somatório em todas equações. Isso lembra uma multiplicação matriz-vetor.\n",
    "E de fato, esse sistema realmente pode ser representado desta forma.\n",
    "\n",
    "Os $r_k(P_i)$, $i = 1, \\dots, n$ serão as componentes de um vetor $\\pi^k$ e os \"pesos\" $|P_i|$, $i = 1,\\dots, n$\n",
    "farão parte da matriz de coeficientes.\n",
    "\n",
    "Visto que há condições impostas aos todos $r_k(P_i)$ $i=1,\\dots,n$, $\\pi^k$ também as terá. Primeiramete, temos que\n",
    "$r_k(P_i) = \\pi_i^k > 0$ o que implica em $\\pi^k > 0$. E também que como $\\sum_{i=1}^n r_k(P_i) = \\sum_{i=1}^n\n",
    "|r_k(P_i)| = 1$ isso significa que $\\|\\pi^k\\|_1 = 1$. Note que $\\pi^k$ é um _vetor de distribuição de probabilidade_.\n",
    "\n",
    "Agora você leitor, consegue lembrar de alguma matriz que era composta por \"pesos\", que juntos somavam a $1$ e que\n",
    "dependiam somente do quanto de _links_ uma página $P_i$ possuía para outras páginas? Essa é justo a matriz $H$.\n",
    "Para lembrança, a matriz $H$ do exemplo dado na seção \"A Matriz Google\" é\n",
    "\n",
    "$$ \n",
    "H = \n",
    "\\begin{bmatrix}\n",
    "            0&1&0&0&0&0&0 \\\\\n",
    "            0&0&1&0&0&0&0\\\\\n",
    "            1/3&0&0&1/3&0&0&1/3\\\\\n",
    "            0&0&0&0&1&0&0\\\\\n",
    "            0&0&0&0&0&1&0\\\\\n",
    "            0&0&0&1&0&0&0\\\\\n",
    "            0&0&0&0&0&0&0\n",
    "\\end{bmatrix}\n",
    ". \n",
    "$$\n",
    "\n",
    "O que nos falta agora é se decidir se a multiplicação será dada pela direita, $\\pi^{k+1} = H\\pi^k$, ou pela\n",
    "esquerda, $(\\pi^t)^{k+1} = (\\pi^t)^kH$. Como os elementos de uma coluna $i$ de $H$ representam as probabilidades do\n",
    "usuário na página da linha $j$ ir a página $i$, esses são justos os \"pesos\" da fórmula. Assim, a multiplicação\n",
    "matriz-vetor deve ser feito pela esquerda.\n",
    "\n",
    "$$\n",
    "(\\pi^t)^{k+1}\n",
    "=\n",
    "(\\pi^t)^kH . \n",
    "$$\n",
    "\n",
    "Porém, como já foi discutido na seção \"A Matriz Google\", sabemos que a utilização da matriz $H$ trás problemas para o\n",
    "modelo. Portanto, em vez de $\\pi^k$ ser multiplicado por $H$, ele será multiplicado por $G$. Desse modo tem-se,\n",
    "\n",
    "$$\n",
    "(\\pi^t)^{k+1}\n",
    "=\n",
    "(\\pi^t)^kG . \n",
    "$$\n",
    "\n",
    "Uma implicação \"ruim\" do uso da $G$ na equação é que ela não nos dará o \"real\" ranqueamento de cada página. Isso se\n",
    "deve ao fato de $G$ ser o resultado de uma soma entre $S$ e $ee^t$. A matriz $S$ preserva a estrutura de links das\n",
    "páginas da Internet e lida de forma \"especial\" com páginas sem links. Enquanto $ee^t$, que é uma matriz de\n",
    "teleportação aleatória, não preserva a estrutura. Ainda assim é necessário a utilização de $G$ para, pelo menos,\n",
    "obter uma aproximação do _rank_ \"real\" das páginas. Não é possível de se calcular este _rank_ \"real\", utilizando\n",
    "somente $S$. Apenas o uso dela pode acarretar no fenômeno do usuário fictício seguir um caminho periódico\n",
    "previsível entre as páginas, algo visto na seção anterior. Ou, de  forma mais geral, o usuário pode navegar somente\n",
    "num conglomerado de páginas sem ter a possibilidade do mesmo conseguir  \"chegar\" em alguma página fora desse\n",
    "conglomerado. Em qualquer um desses casos, o _PageRank_ das páginas que não pertencem a esse conglomerado iria\n",
    "tender a zero conforme o cálculo dos _ranks_ for sendo executado. O que implica na falha do modelo para o cálculo\n",
    "do _rank_ dessas páginas, algo não desejado.\n",
    "\n",
    "Esperamos que os elementos de $\\pi^k$, $\\pi_i^k = r_k(P_i)$, convirjam para um valor estável depois de várias\n",
    "iterações, isso significa que para um $k$ \"grande\", $\\pi^k \\approx \\pi$, para certo $\\pi$. Assim podemos escrever\n",
    "que para $k \\rightarrow \\infty$, $\\pi^k = \\pi$ em que $\\pi$ que é chamado de o _vetor do PageRank_ satisfaz \n",
    "$$\n",
    "\\pi^t\n",
    "=\n",
    "\\pi^tG. \n",
    "$$\n",
    " \n",
    "Deste modo, o problema do _PageRank_ se reduz a encontrar o _vetor probabilidade_ $\\pi$ que satisfaz as condições $\\pi > 0$,\n",
    "$\\|\\pi\\|_1$ = 1 e também $\\pi^t = \\pi^tG$.\n",
    "\n",
    "[Aqui seria bom discutir o fato de não sabermos se o vetor $\\pi$ existe, se ele é único e também se o método\n",
    "proposto para calculá-lo \"funciona\", se ele converge ou não.]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "574371f7",
   "metadata": {},
   "source": [
    "## Autovalor e Autovetor\n",
    "\n",
    "Agora antes de continuar, vamos fazer uma pequena pausa no racicíonio para relembrar o conceito de autovalor e\n",
    "autovetor que será importante para o entendimento deste texto.\n",
    "\n",
    "Você provavelmente está acostumado com a seguinte definição de autovalor e autovetor: dada uma matriz qualquer $A\n",
    "\\in \\mathbb{R}^{n \\times n}$ e um vetor $v \\in \\mathbb{R}^n$, com $v \\neq 0$, $v$ é autovetor de $A$ associado ao\n",
    "autovalor $\\lambda$ se $$ Av = \\lambda v,$$ para algum $\\lambda \\in \\mathbb{R}$. Uma forma de se calcular os\n",
    "autovalores $\\lambda$ é pelo _polinômio característico_ de $A$, $p(A) = det(A-\\lambda I)$.\n",
    "\n",
    "Porém, mesmo A possuindo apenas entradas reais, as raízes de $p(A)$ podem assumir valores complexos. Assim, se nos\n",
    "restringirmos a valores reais para os autovalores $\\lambda$, é possível que haja a \"perda\" de certos $\\lambda$.\n",
    "Isso implicaria que a \"quantidade\", multiplicidade algébrica, de autovalores de $A$ seja menor do que $n$, sua\n",
    "dimensão, o que implicaria em problemas para nossa análise. Portanto, para que nenhum $\\lambda$ fique de fora, será\n",
    "adotada a seguinte definição de autovalor e autovetor:\n",
    "\n",
    "Dada uma matriz $A \\in \\mathbb{C}^{n \\times n}$ e um vetor $v \\in \\mathbb{C}^n$, com $v \\neq 0$, $v$ é autovetor de\n",
    "$A$ associado ao autovalor $\\lambda$ se $$ Av = \\lambda v,$$ para algum $\\lambda \\in \\mathbb{C}$.\n",
    "\n",
    "O conjunto de autovalores $\\lambda$ que uma matriz $A$ possui será chamado de _espectro_ de A, denotado por\n",
    "$\\sigma(A)$.  O valor absoluto dos maiores $\\lambda$ em módulo, $|\\lambda _i| \\geq |\\lambda _j|$, será chamado de\n",
    "_raio espectral_, denotado por $\\rho(A)$. \n",
    "\n",
    "Uma observação importante é que existem tanto autovetor à direita, $Ax = \\lambda x$, quanto à esquerda, $y^tA =\n",
    "\\lambda y^t$. Os autovetores à direita são os que estamos mais habituados a lidar. Mas não se preocupe, se você\n",
    "entende autovetores à direita você  também entende o à esquerda. É praticamente a mesma ideia. Algo não difícil de\n",
    "provar é que o espectro $\\sigma(A)$ de uma matriz A, o conjunto dos autovalores de uma matriz, dos autovetores à\n",
    "direita e dos à esquerda de $A$ é o mesmo (tente provar você mesmo!), assim, se existem autovetores à direita para\n",
    "um certo $\\lambda$ irá também existir autovetores à esquerda para o mesmo $\\lambda$. Porém há sim, certas\n",
    "diferenças entre ambos, como por exemplo que se um vetor $v$ é autovetor à direita associado a um autovalor\n",
    "$\\lambda$ isso não implica que $v^t$ também será autovetor à esquerda associado a $\\lambda$.\n",
    "\n",
    "Dito isso e olhando a equação que chegamos na seção anterior, $\\pi^t = \\pi^tG$, sabemos agora que o problema do\n",
    "_PageRank_ é também um problema de autovalor e autovetor. No caso, queremos encontrar o autovetor à esquerda de\n",
    "$G$, que satisfaz certas condições, associado ao autovalor 1.\n",
    "\n",
    "Como tanto $\\pi$, $\\lambda = 1$ e $G$ pertencem aos reais, temos que todas as iterações $\\pi^k$ pertencerão aos\n",
    "reais.  Assim, mesmo aparecendo números complexos na nossa nova definição de autovalor e autovetor, só utilizaremos\n",
    "números reais para chegar ao _vetor do PageRank_ $\\pi$. Portanto não precisamos nos preocupar com a aritmética\n",
    "complexa no problema do _PageRank_.\n",
    "\n",
    "Agora que abordamos o conceito de autovalores e autovetores, podemos começar a encontrar respostas para os\n",
    "questionamentos a respeito do _vetor de probabilidade_ $\\pi$. Primeiramente, para discutir sobre a existência e unicidade,\n",
    "utilizaremos um teorema importante na área de Álgebra Linear, o chamado teorema de Perron."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee1de62a",
   "metadata": {},
   "source": [
    "## Perron\n",
    "\n",
    "O teorema de Perron infere propriedades de um autovalor específico de uma matriz $A$ que satisfaz apenas duas\n",
    "condições: A é quadrada, $A \\in \\mathbb{R}^{n \\times n}$, e que todos seus elementos sejam positivos, $a_{ij} > 0,\n",
    "i,j = 1,2,\\dots,n$.\n",
    "\n",
    "Dentre todas as afirmações que o teorema diz, as mais importantes para nós são as seguintes: Seja A uma matriz\n",
    "quadrada com apenas entradas positivas. Existe um único autovetor $p$ à direita de $A$, com $p > 0$ e $\\|p\\|_1 =\n",
    "1$, chamado _vetor de Perron_, associado a um autovalor $\\lambda = r > 0$, chamado de _raiz de Perron_, com\n",
    "multiplicidade algébrica igual à 1. Além de $p$ ser único, ele ainda possui a \"unicidade\" de, exceto de seus\n",
    "múltiplos positivos, não haver outros autovetores positivos à direita de $A$, independentemente do autovalor. E,\n",
    "além de tudo isso, $r$ é o maior autovalor em magnitude de $A$, com $|r| = \\rho(A)$. \n",
    "\n",
    "No teorema está sendo apenas descrito o _autovetor de Perron_ à direita tal que $Ap = rp$. Porém ainda assim existe\n",
    "o _autovetor de Perron_ à esquerda, digamos $q$, com as mesmas condições e propriedades de $p$ associado também a\n",
    "_raiz de Perron_ $r$ em que $q^tA = rq^t$.\n",
    "\n",
    "\n",
    "A _matriz Google_ $G$ satisfaz as duas condições iniciais do teorema, assim deve existir um autovalor $\\lambda = r$\n",
    "de $G$ que possui as propriedades descritas acima. O que nos resta agora é encontrá-lo.\n",
    "\n",
    "Note que a multiplicação entre $G$ e $e$, o vetor coluna com todas entradas iguais a 1, representa a soma dos\n",
    "elementos das linhas de $G$, que por construção é igual à 1. Deste modo temos que,  \n",
    "$$ Ge = e. $$\n",
    "\n",
    "Esse resultado implica que o vetor $e$ é um autovetor à direita de $G$ com autovalor $\\lambda = 1$. Sabemos pelo\n",
    "Teorema de Perron que autovetores positivos devem ter como autovalor associado $\\lambda = r$. Como $e> 0$ e tem\n",
    "como autovalor associado $\\lambda = 1$, concluímos que $r = 1$.\n",
    "\n",
    "Agora que sabemos que o _autovalor de Perron_ de $G$ é 1, podemos nos apropriar das propriedades do teorema. Como\n",
    "o _vetor do PageRank_ $\\pi$ satisfaz\n",
    "$$ \\pi^t = \\pi^tG,\\; \\pi > 0 \\; e \\; \\|\\pi\\|_1 = 1,$$\n",
    "temos que $\\pi$ é o _vetor de Perron_ à esquerda de $G$. Assim, utilizando $G$ para se calcular o vetor $\\pi$, sua\n",
    "existência e unicidade são garantidos graças ao Teorema de Perron.\n",
    "\n",
    "Como agora sabemos que o _vetor de probabilidade_ $\\pi$ existe e é único, só nos resta ainda um questionamento: é\n",
    "possível calcular $\\pi$ pelo método de iterações proposto?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9fa0808a",
   "metadata": {},
   "source": [
    "## Método da Potência\n",
    "\n",
    "O método de iterações proposto na seção Fórmula para o PageRank é um caso específico do Método da Potência. O\n",
    "Método da Potência é uma técnica iterativa usada para determinar o autovalor dominanante de uma matriz, isto é, o\n",
    "maior autovalor em magnitude. O método, além de calcular um autovalor, obtém também um autovetor\n",
    "associado.\n",
    "\n",
    "Para poder aplicar o Método da Potência, devemos ter uma matriz $A_{n \\times n}$ diagonalizável (possui $n$\n",
    "autovetores linearmente independentes) com um maior autovalor em magnitude $\\rho(A) = |\\lambda_1|$, de tal forma que\n",
    "$$ \n",
    "|\\lambda_1| > |\\lambda_2| \\geq |\\lambda_3| \\geq \\dots \\geq |\\lambda_n|.  \n",
    "$$\n",
    "Observe que o fato de $|\\lambda_1| > |\\lambda_i|$ para $i = 2, \\dots, n$ impõe que $\\lambda_1 \\in \\mathbb{R}$. Caso\n",
    "a parte imaginária de $\\lambda_1$ fosse diferente de zero, existiria um autovalor conjugado $\\bar{\\lambda}_1 \\neq\n",
    "\\lambda_1$ de tal forma que $|\\bar{\\lambda}_1| = |\\lambda_1|$, que quebraria a hipótese.\n",
    "\n",
    "Seja $\\{V^{(1)}, V^{(2)}, V^{(3)}, \\dots, V^{(n)}\\}$ um conjunto de $n$ autovetores linearmente independentes\n",
    "associados aos seus respectivos autovalores $\\lambda_i$. Podemos representar um vetor $x$ qualquer utilizando os\n",
    "autovetores $V^{(i)}$'s como base. Assim,\n",
    "$$\n",
    "x = \\sum_{i = 1}^n \\alpha_i V^{(i)}.\n",
    "$$\n",
    "Aplicando a matriz $A$ a $x$ obtemos,\n",
    "$$\n",
    "Ax = \\sum_{i = 1}^n \\alpha_i AV^{(i)} = \\sum_{i = 1}^n \\lambda_i \\alpha_i V^{(i)}.\n",
    "$$\n",
    "Aplicando a matriz $A$ a $Ax$ obtemos,\n",
    "$$\n",
    "AAx = A^2x = \\sum_{i = 1}^n \\lambda_i \\alpha_i AV^{(i)} = \\sum_{i = 1}^n \\lambda_i^2 \\alpha_i V^{(i)}.\n",
    "$$\n",
    "Fazendo esse mesmo processo $k$ vezes chegamos em,\n",
    "$$\n",
    "AA^{k-1}x = A^kx = \\sum_{i = 1}^n \\lambda_i^{k-1} \\alpha_i AV^{(i)} = \\sum_{i = 1}^n \\lambda_i^k \\alpha_i V^{(i)}.\n",
    "$$\n",
    "Fatorando $\\lambda_1^k$ do lado direito da última equação vemos que,\n",
    "$$\n",
    "A^kx = \\lambda_1^k \\sum_{i = 1}^n \\left(\\frac{\\lambda_i}{\\lambda_1}\\right)^k \\alpha_i V^{(i)},\n",
    "$$\n",
    "e assim, chegamos em,\n",
    "$$\n",
    "A^kx = \\lambda_1^k \\left(\\alpha_1 V^{(1)} + \\left(\\frac{\\lambda_2}{\\lambda_1}\\right)^k\n",
    "\\alpha_2 V^{(2)} + \\dots + \\left(\\frac{\\lambda_n}{\\lambda_1}\\right)^k \\alpha_n V^{(n)} \\right).\n",
    "$$\n",
    "Observe que como $|\\lambda_1| > |\\lambda_i|$ para $i = 2,\\dots,n$, temos que $(\\lambda_i/\\lambda_1) < 1$ e assim,\n",
    "$$\n",
    "\\lim_{k \\to \\infty} \\left(\\frac{\\lambda_i}{\\lambda_1}\\right)^k = 0, \\; \\; \\text{para } i = 2, \\dots, n.\n",
    "$$\n",
    "Portanto conforme aumenta o número de iterações $k$, os termos de $\\lambda_1^k \\sum_{i=1}^{n}\n",
    "\\left(\\frac{\\lambda_i}{\\lambda_1}\\right)^k \\alpha_i V^{(i)}$ vão tendendo cada vez mais a $\\lambda_1^k \\alpha_1 V^{(1)}$. Para que\n",
    "possamos tomar o limite de $k$ tendendo ao infinito, devemos tomar o cuidado para que $\\lambda_1^k$ convirja. Para\n",
    "isso, só nos resta a opção $\\lambda_1 = 1$. Caso $|\\lambda_1|$ fosse menor do que 1, teríamos $\\lim_{k \\to \\infty}\n",
    "\\lambda_1^k = 0$, que não nos interessa. Caso $|\\lambda_1| > 1$, o limite $\\lim_{k \\to \\infty}\\lambda_1^k$ iria\n",
    "divergir para o infinito. E se caso $\\lambda_1 = -1$, $\\lambda_1^k$ não convergiria para um valor fixo conforme\n",
    "$k$ aumenta. Assim, assumindo $\\lambda_1 = 1$ e tomando o limite de $k$ tendendo ao infinito, vemos que,\n",
    "$$\n",
    "\\lim_{k \\to \\infty}A^kx =  \\lim_{k \\to \\infty} \\lambda_1^k \\left(\\alpha_1 V^{(1)} + \\left(\\frac{\\lambda_2}{\\lambda_1}\\right)^k\n",
    "\\alpha_2 V^{(2)} + \\dots + \\left(\\frac{\\lambda_n}{\\lambda_1}\\right)^k \\alpha_n V^{(n)} \\right) = \\lim_{k \\to \\infty} \n",
    "\\lambda_1^k \\alpha_1 V^{(1)}\n",
    "$$\n",
    "e assim,\n",
    "$$\n",
    "\\lim_{k \\to \\infty}A^kx = \\alpha_1 V^{(1)}.\n",
    "$$\n",
    "\n",
    "Portanto, chegamos na conclusão que, pela equação acima, $A^kx$ tende a um par de autovalor-autovetor, com\n",
    "autovalor $\\lambda = 1$ e autovetor associado $V = \\alpha_1 V^{(1)}$ sempre que $A$ for diagonalizável e tiver\n",
    "$\\lambda_1 = 1$ como o maior autovalor em módulo. A única forma do limite dar um resultado ``insatisfatório\", é\n",
    "caso $\\alpha_1 = 0$, isto é, a contribuição de $V^{(1)}$ na representação de $x$ pela base $\\left\\{V^{(1)},\n",
    "V^{(2)}, \\dots, V^{(n)}\\right\\}$ for zero. Contudo, se isso acontecesse, o que é raro, poderíamos apenas usar\n",
    "algum outro $x_0$ que tenha $\\alpha_1 \\neq 0$ em $A^kx_0$.  \n",
    "\n",
    "Desenvolvemos até aqui o Método da Potência para $\\lambda_1 = 1$, que é o caso que justamente nos interessa. No\n",
    "caso que $\\lambda_1 \\neq 1$, o Método da Potência é levemente diferente. Nesse, $A^kx$ é normalizada a cada\n",
    "iteração, com o intuito de não permitir que tende a zero ou divirja.  Todavia, a ideia por trás e o resultado final\n",
    "obtido será análogo ao que foi desenvolvido.\n",
    "\n",
    "Aplicando o resultado acima para a matriz $G$ e supondo que ela seja diagonalizável, devemos analisar os maiores\n",
    "autovalores em magnitude de $G$ para se certificar que o Método da Potência aplicada a ela converge. Já sabemos da\n",
    "seção Perron que $\\lambda_1 = 1 = \\rho(G)$, que implica que $1 \\geq |\\lambda_2| \\geq \\dots \\geq |\\lambda_n|$.\n",
    "Porém, deve-se ter $\\lambda_1 = 1$ como o único autovalor no raio espectral, isto é, $1 > |\\lambda_i|$ para $i =\n",
    "2,3, \\dots, n$. É possível provar, no entanto, não o faremos aqui, que o parâmetro $\\alpha \\in (0,1)$ da equação\n",
    "que define $G$, \n",
    "$$\n",
    "G = \\alpha S + (1 - \\alpha) ee^t, \n",
    "$$\n",
    "é o segundo maior autovalor em magnitude de $G$. Como $\\lambda_2 = \\alpha < 1$, temos que $1 > \\alpha \\geq\n",
    "|\\lambda_3| \\geq \\dots \\geq |\\lambda_n|$.\n",
    "\n",
    "Como $G$ satisfaz todas as hipóteses para a convergência do Método da Potência, temos que o limite $\\lim_{k \\to\n",
    "\\infty} G^k x$, converge para um autovetor $V$ associado ao autovalor $\\lambda = 1$, assim\n",
    "$$\n",
    "\\lim_{k \\to \\infty} G^kx = V\n",
    "$$\n",
    "\n",
    "Como $\\pi$ e $V$ são autovetores associados de $\\lambda = 1$ que, devido ao Teorema de Perron, possui\n",
    "multiplicidade algébrica igual a 1, sabemos que $V$ é um múltiplo de $\\pi$. Devido ao fato de $\\|\\pi\\|_1 = 1$, \n",
    "$\\pi > 0$ e $V$ ser $V > 0$ ou $V < 0$, temos que\n",
    "\n",
    "$$\n",
    "\\pi = \\text{sgn}(V) \\frac{V}{\\|V\\|_1},\n",
    "$$\n",
    "\n",
    "em que $\\text{sgn}(V)$ = 1 caso $V > 0$ e $\\text{sgn}(V)$ = -1 caso $V < 0$.\n",
    "\n",
    "Assim, chegamos na conclusão que o _vetor do PageRank_ $\\pi$ é calculável pelo Método da Potência aplicado a $G$.\n",
    "\n",
    "\n",
    "[Cadeia de Markov $\\rightarrow \\dots \\rightarrow$ Método da Potência $\\rightarrow \\dots$]\n",
    "\n",
    "[Método da Potência: Supor G diagonalizável ?]\n",
    "\n",
    "[dúvida: já chamei $\\pi$ de vetor do PageRank ?]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4f7fcf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "Cell": {
   "cm_config": {
    "lineWrapping": true
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
