{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a47da10c",
   "metadata": {},
   "source": [
    "<div hidden>\n",
    "$\\renewcommand{\\v}{\\nu}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913ea05e",
   "metadata": {},
   "source": [
    "# Introdução\n",
    "\n",
    "(Muitos dos quais lerão esse texto, raramente em sua vida, se depararam com o problema de ter que utilizar outro\n",
    "site de busca, além do Google, para encontrar algo que procura na internet. Porém, quando ela ainda estava em sua\n",
    "fase “jovem”, isso era um problema comum.)\n",
    "    \n",
    "Praticamente todo mundo que você conhece usa o Google. Uns ou outros utilizam o Bing e algumas pessoas perdidas no\n",
    "tempo usam o Yahoo!. Mas, algo razoável de se perguntar é: sempre foi assim? Provavelmente a resposta a essa\n",
    "pergunta é: Nem sempre. [posso reler a introdução do livro para ter ideias boas do que adicionar aqui.] | Mas\n",
    "então, **por que** o Google é _o Google_? A resposta disso em uma palavra é: PageRank.\n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "## PageRank\n",
    "    \n",
    "PageRank é um algoritmo criado por Sergey Brin e Larry Page, os fundadores da Google, no final de década de 90, que\n",
    "utiliza a estrutura de _links_ da Internet para dar uma classificação a suas páginas.\n",
    "\n",
    "A ideia central do algoritmo é resumida na seguinte frase: uma página é importante se páginas importantes levam a\n",
    "ela.  Em minha opinião, isso parece um pouco recursivo demais. Uma página será importante se outras páginas que\n",
    "ligam a ela forem importantes. Agora, essas páginas são importantes porque outras páginas importantes ligam a elas.\n",
    "Seguindo nesse raciocínio, chegamos a seguinte pergunta: quem são \"as primeiras\" páginas importantes? Essa\n",
    "recursão, na verdade, é algo embutido no algoritmo, algo que é trabalhado para se calcular a classificação de cada\n",
    "página. Conforme o texto, mais especificamente na seção de matemática, for progredindo, essa ideia ficará mais\n",
    "clara e veremos que, na verdade, não precisam existir \"as primeiras\" páginas importantes.\n",
    "\n",
    "Um fato curioso sobre o PageRank é que ele utiliza, praticamente, somente conceitos básicos da Álgebra Linear. Sim,\n",
    "um dos grandes motivos do porquê a _Google_ ser tão poderosa no mercado tecnológico de hoje em dia é devido, no seu\n",
    "início como empresa, à implementação de Álgebra Linear básica em algoritmos. Qualquer um que já fez algum curso de\n",
    "Álgebra Linear possui as ferramentas básicas para entender _como_ e o _por que_ o PageRank funciona. Esse é justo o\n",
    "foco central deste texto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcdfd1c",
   "metadata": {},
   "source": [
    "# A Matemática por trás\n",
    "\n",
    "## Cadeias de Markov\n",
    "\n",
    "Inicialmente, discutiremos a respeito de Cadeias de Markov. Elas o ajudarão no entendimento e a ligar conceitos ao\n",
    "longo do texto. Você provavelmente não conhece nada a respeito do que é uma Cadeia de Markov. Portanto, aqui só\n",
    "introduziremos seus conceitos fundamentais.\n",
    "\n",
    "É verdade que, conforme comentado acima, apenas o conhecimento básico de Álgebra Linear já é suficiente para o\n",
    "entendimento de como funciona o _PageRank_. Porém, conhecer os fundamentos de Cadeias de Markov o ajuda a entender\n",
    "uma área da matemática, provavelmente nova para você, leitor, e ter uma melhor noção \"do que está acontecendo\". E,\n",
    "além do mais, não machuca ninguém.\n",
    "\n",
    "Uma _variável aleatória_ é uma função que designa resultados de um evento a valores numéricos. Seja $\\Omega$ o conjunto de\n",
    "todas os possíveis resultados que podem acontecer de um evento. Deste modo, definindo de um jeito um pouco mais formal, uma\n",
    "variável aleatória é uma função tal que $X: \\Omega \\rightarrow A$, com $A \\subset \\mathbb{R}$. Por exemplo, o lançamento de\n",
    "uma moeda possuí dois possíveis resultados, pode haver de sair cara, ou de sair coroa do lançamento. Assim, temos que $\\Omega\n",
    "= \\{\\text{cara}, \\text{coroa} \\}$. Podemos definir a variável aleatória $X$ de tal modo que caso o resultado do lançamento da\n",
    "moeda for cara, $X(\\text{cara}) = 1$, e se caso sair coroa, $X(\\text{coroa}) = 0$.\n",
    "\n",
    "Um _processo estocástico_ é um conjunto de variáveis aleatórias $\\left\\{X_t \\right\\}_{t=0}^{\\infty}$\n",
    "que possuem um conjunto de possíveis valores em comum $S = \\left\\{S_1, S_2, \\dots, S_n\\right\\}$, que é chamado de\n",
    "_espaço de estados_ para o processo. O parâmetro $t$ é geralmente pensado como tempo, e $X_t$ representa o estado\n",
    "do processo no tempo $t$.  Por exemplo, considere o processo de navegar na Internet clicando em links que vão de\n",
    "uma página a outra. O espaço de estados $S$ é o conjunto de todas a páginas da Internet e a variável aleatória $X_t$\n",
    "é a página visitada no tempo $t$.\n",
    "\n",
    "Uma _Cadeia de Markov_ é um processo estocástico com a propriedade de ser um processo \"sem memória\". A\n",
    "probabilidade da cadeia ir para algum estado depende somente do estado atual que ela se encontra e não dos\n",
    "estados anteriores. Escrevendo isso em forma de equação temos,  \n",
    "$$\n",
    "P\\left(X_{t+1}=S_j | X_t=S_{i_t},X_{t-1}=S_{i_{i-1}},\\dots,X_0=S_{i_0}\\right) = P\\left(X_{t+1}=S_j | X_t=S_{i_t}\\right)\n",
    "$$\n",
    "para cada $t = 0, 1, 2, \\dots$. A notação $P\\left(E | F\\right)$ representa a probabilidade condicional do evento\n",
    "$E$ ocorrer dado que $F$ ocorreu. Por exemplo, é uma cadeia de Markov o processo de um usuário navegar na Internet\n",
    "selecionando links que pertencem a página atual em que o usuário se encontra.\n",
    "\n",
    "A _probabilidade de transição_ $p_{ij} = P\\left(X_t=S_j | X_{t-1}=S_i\\right)$ é a probabilidade de estar no estado\n",
    "$S_j$ no tempo $t$ com o fato da cadeia ter passado no estado $S_i$ em $t-1$. Assim, por exemplo, pensando no contexto de\n",
    "páginas da Internet, $p_{ij}$ é a probabilidade de um usuário estar na página $j$ dado que, há um momento atrás, ele estava\n",
    "na página $i$.\n",
    "\n",
    "A _matriz de probabilidade de transição_ é definida como sendo uma matriz em que seus elementos são probabilidades\n",
    "de transição, $P_{n \\times n}(t) = \\left[p_{ij}(t)\\right]$. Ela é não-negativa, visto que seus elementos são todos\n",
    "probabilidades. Nota-se também que a soma dos elementos de cada linha deve ser 1, já que a soma das probabilidades\n",
    "de ir para qualquer estado pertencente ao espaço de estados $S$ deve-se somar a 1. Matrizes que satisfazem essas\n",
    "propriedades, de serem não-negativas e que a soma dos elementos de cada linha ser igual a 1, são chamadas de\n",
    "_matrizes estocásticas_. Portanto, dado um tempo $t$ específico, $P(t)$ é uma matriz estocástica.  \n",
    "\n",
    "Uma _Cadeia de Markov_ é _estacionária_ se as probabilidades de transição não variarem com o tempo, isto é $p_{ij}(t) =\n",
    "p_{ij}$ para todo $t$.\n",
    "\n",
    "Um _vetor de distribuição de probabilidade_ (ou apenas \"vetor de probabilidade\") é definido com sendo um vetor $p^t\n",
    "= (p_1, p_2, \\dots, p_n)$ não-negativo de tal forma que a soma de seus elementos é igual a 1, ou seja, $\\|p\\|_1 = 1$. \n",
    "\n",
    "Um _vetor de distribuição de probabilidade estacionário_ para uma Cadeia de Markov estacionária que tem $P$ como matriz de\n",
    "transição é um vetor $\\pi$ de tal forma que $\\pi^t = \\pi^t P$.\n",
    "\n",
    "Agora que conhecemos alguns novos conceitos de Cadeias de Markov, vamos abordar o problema geral do PageRank."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d9acbf",
   "metadata": {},
   "source": [
    "## A Matriz Google\n",
    "\n",
    "Para começarmos, suponha um conjunto $P$ com $n$ páginas da Internet dadas por $P_i \\ (i = 1,2,\\cdots,n)$. Suponha\n",
    "também que as páginas desse conjunto possuem _links_ que vão para páginas do mesmo conjunto. Uma forma interessante\n",
    "de visualizar as páginas de $P$ e as ligações entre elas é por meio de um grafo. Por motivos didáticos, usaremos um\n",
    "conjunto $P$ com $n = 7$ páginas dadas pelo grafo abaixo.\n",
    "\n",
    "![Grafo](Imagens/Grafo_1.png) \n",
    "\n",
    "Na imagem, os nós (círculos) representam as páginas e as arestas (setas) representam as ligações entre as páginas.\n",
    "\n",
    "Podemos representar esse grafo em um formato matricial. Seja $A$ uma matriz tal que $a_{ij}$ = 1, se a página $i$\n",
    "possuí um link para a página $j$, e $a_{ij}$ = 0 caso contrário (a página $i$ não possui um _link_ para a página\n",
    "$j$).  Acabamos de criar uma chamada matriz de _Adjacência_ do grafo. Assim, a matriz $A$ do grafo do conjunto de\n",
    "páginas $P$ é dada por\n",
    "\n",
    "$$ A = \\begin{bmatrix} \n",
    "            0&1&0&0&0&0&0\\\\\n",
    "            0&0&1&0&0&0&0\\\\\n",
    "            1&0&0&1&0&0&1\\\\\n",
    "            0&0&0&0&1&0&0\\\\\n",
    "            0&0&0&0&0&1&0\\\\\n",
    "            0&0&0&1&0&0&0\\\\\n",
    "            0&0&0&0&0&0&0\n",
    "\\end{bmatrix} . $$\n",
    "\n",
    "Agora, vamos olhar para a matriz $A$ de uma forma diferente. E se o elemento $a_{ij}$ representasse a probabilidade\n",
    "de um usuário da Internet ir à página $j$ considerando o fato dele estar, agora no momento, na página $i$?\n",
    "Observando $A$, vemos que essa interpretação nova não está consoante com a matriz e que um problema já visível está\n",
    "em sua linha 3. Segundo nossa interpretação, se um usuário estiver na página 3, a probabilidade dele ir para página\n",
    "1 é igual à 1. Porém, a chance dele ir para as páginas 4 e 7 também é 1, algo que não faz sentido. Um modo de\n",
    "contornar esse problema é criar uma _nova_ matriz que tenha a mesma “cara” de  $A$ e que também, corresponda com\n",
    "essa nova interpretação probabilística.\n",
    "\n",
    "Um modo de criar essa nova matriz, digamos $H$, de forma que, as probabilidades sejam “justas” ou “sem viés” é pela\n",
    "seguinte definição: o elemento $h_{ij}$ é igual à $(\\sum_{k=1}^{n}h_{ik})^{-1}$ se a página $i$ possuí um _link_\n",
    "para a página $j$ e $h_{ij} = 0$ caso contrário. Embora pareça um pouco complicado essa nova definição, saiba que a\n",
    "única diferença entra ela e a de $A$ é que estamos “normalizando” as linhas não-nulas para que a soma entre seus\n",
    "elementos seja igual à 1 e assim, faça sentido pensar nela como probabilidade. Assim, $H$ será dada por,\n",
    "\n",
    "$$ H = \\begin{bmatrix} 0&1&0&0&0&0&0 \\\\\n",
    "            0&0&1&0&0&0&0\\\\\n",
    "            1/3&0&0&1/3&0&0&1/3\\\\\n",
    "            0&0&0&0&1&0&0\\\\\n",
    "            0&0&0&0&0&1&0\\\\\n",
    "            0&0&0&1&0&0&0\\\\\n",
    "            0&0&0&0&0&0&0\n",
    "\\end{bmatrix} . $$\n",
    "\n",
    "A matriz $H$ é uma chamada _matriz subestocástica_, visto que todas suas linhas não-nulas somam a 1. Porém, ainda\n",
    "na matriz $H$ temos um problema. E a linha 7? Ela, por sua vez, possuí uma linha completa de zeros, o que quer\n",
    "dizer pela nossa interpretação que, se um usuário estiver na página 7, a probabilidade dele ir para qualquer outra\n",
    "página (de $P$) é zero. O que intuitivamente quer dizer que ele ficará na página 7 **para sempre**.  Obviamente,\n",
    "isso é algo que não queremos que aconteça com nosso modelo, que a página 7 seja um “buraco negro” para nossos\n",
    "usuários, em que, se eles chegarem lá, viverão para sempre.\n",
    "\n",
    "Faremos o seguinte para contornar esse fato: se uma linha contiver apenas zeros, ela será alterada de forma que,\n",
    "todos seus elementos sejam iguais à $\\frac{1}{n}$, em que $n$ é o número de páginas de $P$ (dimensão de $H$). O que\n",
    "isso interpretativamente faz é que caso um usuário chegue a uma página que não possua ligação alguma com outra, ele\n",
    "se direcionará a alguma página de $P$. É como se ele escolhesse uma página do seu histórico de navegação\n",
    "“aleatoriamente”.\n",
    "\n",
    "Assim, a matriz $H$ “atualizada”, o qual chamaremos de $S$, é dada por\n",
    "\n",
    "$$ S = \\begin{bmatrix} 0&1&0&0&0&0&0 \\\\\n",
    "            0&0&1&0&0&0&0\\\\\n",
    "            1/3&0&0&1/3&0&0&1/3\\\\\n",
    "            0&0&0&0&1&0&0\\\\\n",
    "            0&0&0&0&0&1&0\\\\\n",
    "            0&0&0&1&0&0&0\\\\\n",
    "            1/7&1/7&1/7&1/7&1/7&1/7&1/7 \n",
    "\\end{bmatrix} . $$\n",
    "\n",
    "Recorde que cada elemento de $S$, $s_{ij}$, representa a probabilidade de um usuário ir de uma página $i$ para uma\n",
    "página $j$, assim $s_{ij} = P\\left(X_{t+1} = S_j | X_t = S_i\\right)$, com $X_t$ sendo a página visitada no tempo\n",
    "$t$, $S_j$ e $S_i$ sendo, respectivamente, a página $j$ e a página $i$. Portanto, devido a nossa construção, $S$ é\n",
    "uma _matriz de probabilidade de transição estocástica_ com cada elemento seu $s_{ij}$ sendo uma _probabilidade de\n",
    "transição_ entre páginas.\n",
    "\n",
    "Porém, _incrivelmente_, ainda há mais um problema (e o último) com nosso modelo. Se, você leitor, observasse a\n",
    "matriz $S$ por um tempo suficientemente grande, provavelmente iria notar a seguinte propriedade da matriz: Se um\n",
    "usuário estiver na página 4, ele irá para a página 5. Se estiver na página 5, ele irá para a página 6. E se estiver\n",
    "na página 6, ele irá para a página 4. E assim por diante, para sempre. Criando assim um _loop_ eterno da navegação\n",
    "do mesmo. E com isso, a partir do momento em que entra pela primeira vez em uma dessas páginas, as outras ($P_i$\n",
    "com $i= 1,2,3,7$) efetivamente “não existirão” mais em nosso modelo, dessa maneira, não será possível quantificar\n",
    "algum tipo de classificação numérica para as mesmas, mas apenas para aquelas que estão no ciclo.\n",
    "\n",
    "Para que o fato discutido acima não ocorra, consideraremos mais uma, e última modificação no comportamento do nosso\n",
    "usuário. Agora, antes de simplesmente selecionar um _link_ na página que atualmente se encontra, o usuário terá uma\n",
    "probabilidade $1 - \\alpha$ (com $\\alpha \\in (0,1)$) de ir para uma página qualquer de $P$. Isso, além trazer\n",
    "propriedades que garantirão o êxito de nosso modelo, algo que veremos posteriormente, ela também traz a ele um\n",
    "comportamento esperado de qualquer um que navega a Internet. É razoável esperar de uma pessoa que ela não somente\n",
    "vá sendo levada site a site seguindo apenas os _links_ da página em que se encontra. Ela também pode, por exemplo,\n",
    "entrar em algum site em que a aba está aberta em seu navegador, ou também, abrir um de seu histórico por livre e\n",
    "espontânea vontade.\n",
    "\n",
    "Seja o vetor $e \\in \\mathbb{R}^n$ um vetor coluna com todas entradas iguais a 1 \n",
    "$$ e = \n",
    "\\begin{bmatrix} \n",
    "                1 \\\\\n",
    "                1 \\\\\n",
    "                \\vdots \\\\\n",
    "                1\n",
    "\\end{bmatrix}\n",
    ". \n",
    "$$\n",
    "A nova matriz criada a partir de $S$ será a chamada _matriz Google_ $G$ que é dada pela seguinte equação:\n",
    "\n",
    "$$ G = \\alpha S + \\left(1 - \\alpha\\right)\\frac{1}{n}ee^T . $$\n",
    "\n",
    "Em que $\\frac{1}{n}ee^T \\in \\mathbb{R}^{n \\times n}$ é uma matriz de “teleportação aleatória”, o qual todos seus elementos\n",
    "são iguais à $\\frac{1}{n}$. Em nosso exemplo, escolhendo $\\alpha = 0.85$, a matriz G é\n",
    "\n",
    "$$ G = 0.85 \\begin{bmatrix} 0&1&0&0&0&0&0 \\\\\n",
    "            0&0&1&0&0&0&0\\\\\n",
    "            1/3&0&0&1/3&0&0&1/3\\\\\n",
    "            0&0&0&0&1&0&0\\\\\n",
    "            0&0&0&0&0&1&0\\\\\n",
    "            0&0&0&1&0&0&0\\\\\n",
    "            1/7&1/7&1/7&1/7&1/7&1/7&1/7 \n",
    "\\end{bmatrix}\n",
    "+ 0.15 \\begin{bmatrix} 1/7&1/7&1/7&1/7&1/7&1/7&1/7\\\\\n",
    "            1/7&1/7&1/7&1/7&1/7&1/7&1/7\\\\\n",
    "            1/7&1/7&1/7&1/7&1/7&1/7&1/7\\\\\n",
    "            1/7&1/7&1/7&1/7&1/7&1/7&1/7\\\\\n",
    "            1/7&1/7&1/7&1/7&1/7&1/7&1/7\\\\\n",
    "            1/7&1/7&1/7&1/7&1/7&1/7&1/7\\\\\n",
    "            1/7&1/7&1/7&1/7&1/7&1/7&1/7\n",
    "\\end{bmatrix} . $$\n",
    "\n",
    "Note que, que nem a matriz $S$, $G$ representa uma _matriz de probabilidade de transição estocástica_, com cada\n",
    "elemento seu $g_{ij}$ sendo uma _probabilidade de transição_ entre páginas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6796c0b",
   "metadata": {},
   "source": [
    "## O Vetor do PageRank\n",
    "\n",
    "Seguindo a filosofia de que _uma página é importante se páginas importantes direcionam a ela_, vamos definir uma\n",
    "fórmula para calcular o $PageRank$ de uma página. Uma possível fórmula poderia simplesmente ser a soma dos\n",
    "PageRanks das outras páginas que apontam para ela. Assim sendo, $$ r(P_i) = \\sum_{P_j \\in B_{P_i}}{r(P_j)}, $$ em\n",
    "que $r(P_i)$ é o PageRank da página $i$, $r(P_j)$ o da página $j$ e $B_{P_i}$ é o conjunto das páginas que apontam\n",
    "para a página $i$. Como cada $r(P_i)$ é um _rank_, temos que $r(P_i) > 0$. Para que o valor de algum $r(P_i)$ não\n",
    "\"exploda\", será também imposto a condição de que $\\sum_{i=1}^n r(P_i) = 1$.\n",
    "\n",
    "Porém, se questione do seguinte: imagine duas páginas os quais possuem o mesmo PageRank. Uma dessas páginas possui apenas um\n",
    "link para uma página qualquer, enquanto a outra possui links para cem páginas. A \"importância\" do link da primeira página\n",
    "deve ser o mesmo que a \"importância\" de cada link da segunda?\n",
    "\n",
    "Para os criadores do Google, a resposta é não. Quanto mais links uma página possui para outras páginas, a \"importância\" dada\n",
    "a cada um desses links deve valer menos. Portanto, para a fórmula do PageRank, é preciso ter um fator de peso que mede o quão\n",
    "\"expressivo\" é um link.\n",
    "\n",
    "Deste modo, vamos utilizar a seguinte fórmula:\n",
    "$$\n",
    "r(P_i) = \\sum_{P_j \\in B_{P_i}}\\frac{r(P_j)}{|P_j|}, \n",
    "$$\n",
    "em que $|P_j|$ é o número de links de $P_j$.\n",
    "\n",
    "O problema óbvio com essa fórmula é que simplesmente não sabemos os valores de $r(P_j)$ para calcular $r(P_i)$. A forma\n",
    "utilizada para lidar com isso será aplicar a fórmula sucessivamente para as páginas de $P$, utilizando, a cada nova iteração,\n",
    "os valores obtidos para $r(P_j)$ da iteração prévia e torcer para que os valores de $r(P_j)$ convirjam para algo _estável_\n",
    "após \"várias\" iterações. Assim, introduzindo uma nova notação, temos\n",
    "\n",
    "$$ r_{k+1}(P_i) = \\sum_{P_j \\in B_{P_i}}\\frac{r_k(P_j)}{|P_j|}, $$\n",
    "em que $r_{k+1}(P_i)$ representa a $k + 1$-ésima iteração do _rank_ de $P_i$ e $r_{k}(P_j)$ representa a $k$-ésima iteração\n",
    "do _rank_ de $P_j$.\n",
    "\n",
    "Para que a fórmula acima \"funcione\", é preciso saber quem são os $r_0(P_i)$, $i = 1,\\dots, n$, que é justo nossa dúvida\n",
    "inicial discutida na seção \"PageRank\". Como inicialmente conhecemos nada sobre as páginas $P_i$, é razoável pensar que todas,\n",
    "no começo do processo, valem o mesmo. Portanto a condição inicial dos $r_k(P_i)$ será $r_0(P_i) = \\frac{1}{n}$ para $i =\n",
    "1,\\dots, n$.\n",
    "\n",
    "Vamos ilustrar o funcionamento dessa fórmula utilizando o mesmo conjunto $P$ de páginas da seção \"A Matriz Google\".\n",
    "\n",
    "![Grafo](Imagens/Grafo_1.png)\n",
    "\n",
    "Calculando apenas a primeira iteração para cada página, temos:\n",
    "\n",
    "$$\n",
    "r_1(P_1) = \\frac{r_0(P_3)}{|P_3|} = \\frac{\\frac{1}{7}}{3} = \\frac{1}{21}; \\\\\n",
    "$$\n",
    "$$\n",
    "r_1(P_2) = \\frac{r_0(P_1)}{|P_1|} = \\frac{\\frac{1}{7}}{1} = \\frac{1}{7}; \\\\\n",
    "$$\n",
    "$$\n",
    "r_1(P_3) = \\frac{r_0(P_2)}{|P_2|} = \\frac{\\frac{1}{7}}{1} = \\frac{1}{7}; \\\\\n",
    "$$\n",
    "$$\n",
    "r_1(P_4) = \\frac{r_0(P_3)}{|P_3|} + \\frac{r_0(P_6)}{|P_6|} = \\frac{\\frac{1}{7}}{3} + \\frac{\\frac{1}{7}}{1} = \\frac{4}{21}; \\\\\n",
    "$$\n",
    "$$\n",
    "r_1(P_5) = \\frac{r_0(P_4)}{|P_4|} = \\frac{\\frac{1}{7}}{1} = \\frac{1}{7}; \\\\\n",
    "$$\n",
    "$$\n",
    "r_1(P_6) = \\frac{r_0(P_5)}{|P_5|} = \\frac{\\frac{1}{7}}{1} = \\frac{1}{7}; \\\\\n",
    "$$\n",
    "$$\n",
    "r_1(P_7) = \\frac{r_0(P_3)}{|P_3|} = \\frac{\\frac{1}{7}}{3} = \\frac{1}{21}.\n",
    "$$\n",
    "\n",
    "Veja que, a partir da primeira iteração, o _PageRank_ de algumas páginas já começam se sobressair sobre outras. Caso\n",
    "continuássemos, utilizariamos os $r_1(P_i)$, $i = 1,2,\\dots,7$, como sendo os ranks \"atuais\" das páginas para o cálculo dos\n",
    "$r_2(P_i)$. Depois usaríamos os $r_2(P_i)$ para o cálculo dos $r_3(P_i)$, e assim por diante, até, possívelmente, atingir\n",
    "valores estáveis.  \n",
    "\n",
    "Agora que a mágica começa a acontecer. Voltando ao caso geral de um conjunto de $n$ páginas $P$, por\n",
    "\n",
    "$$ r_{k+1}(P_i) = \\sum_{P_j \\in B_{P_i}}\\frac{r_k(P_j)}{|P_j|}, $$\n",
    "\n",
    "temos $n$ fórmulas que calculam $r_{k+1}(P_1)$, $r_{k+1}(P_2)$, $\\dots$, $r_{k+1}(P_n)$. No cálculo de cada uma, sempre há\n",
    "uma soma de $n$ termos (considerando soma por 0). Cada um desses termos é formado por uma multiplicação entre um _rank_\n",
    "$r_k(P_j)$ e um peso, sendo o mesmo $0$ ou $\\frac{1}{|P_j|}$. Deste modo, esse sistema de equações está parecendo muito uma\n",
    "multiplicação matriz-vetor. E de fato, ele realmente pode ser representado desta forma.\n",
    "\n",
    "Os $r_k(P_i)$, $i = 1, \\dots, n$ serão as componentes de um vetor coluna $\\pi^k$, assim $r_k(P_1) = \\pi^k_1$, $r_k(P_2) =\n",
    "\\pi^k_2$, etc, e os pesos $\\frac{1}{|P_i|}$ farão parte da matriz de coeficientes.\n",
    "\n",
    "Agora você leitor, consegue lembrar de alguma matriz que era composta por pesos, que juntos somavam a $1$ e que\n",
    "dependiam somente do quanto de _links_ uma página $P_i$ possuía para outras páginas? Essa é justo a matriz $H$.\n",
    "Para lembrança, a matriz $H$ do exemplo dado na seção \"A Matriz Google\" é\n",
    "\n",
    "$$ \n",
    "H = \n",
    "\\begin{bmatrix}\n",
    "            0&1&0&0&0&0&0 \\\\\n",
    "            0&0&1&0&0&0&0\\\\\n",
    "            1/3&0&0&1/3&0&0&1/3\\\\\n",
    "            0&0&0&0&1&0&0\\\\\n",
    "            0&0&0&0&0&1&0\\\\\n",
    "            0&0&0&1&0&0&0\\\\\n",
    "            0&0&0&0&0&0&0\n",
    "\\end{bmatrix}\n",
    ". \n",
    "$$\n",
    "\n",
    "Como agora sabemos a matriz e o vetor que estamos lidando, falta se decidir se a multiplicação será dada pela direita,\n",
    "$\\pi^{k+1} = H\\pi^k$, ou pela esquerda, $(\\pi^t)^{k+1} = (\\pi^t)^kH$ ($\\pi^t$ representa o vetor transposto de $\\pi$).\n",
    "Utilizando o mesmo exemplo do grafo anterior, vemos que a multiplicação entre $\\pi^0$ e $H$ deve ser feita pela esquerda para\n",
    "\"bater\" com o sistema de equações dos $r_1(P_i)$ calculado acima.\n",
    "\n",
    "$$\n",
    "\\displaystyle\\left(\\pi^t\\right)^1 = \\displaystyle\\left(\\pi^t\\right)^0H \\\\\n",
    "\\begin{bmatrix}\n",
    "r_{1}(P_1)&r_{1}(P_2)&r_{1}(P_3)&r_{1}(P_4)&r_{1}(P_5)&r_{1}(P_6)&r_{1}(P_7)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{7}&\\frac{1}{7}&\\frac{1}{7}&\\frac{1}{7}&\\frac{1}{7}&\\frac{1}{7}&\\frac{1}{7}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "            0&1&0&0&0&0&0 \\\\\n",
    "            0&0&1&0&0&0&0\\\\\n",
    "            1/3&0&0&1/3&0&0&1/3\\\\\n",
    "            0&0&0&0&1&0&0\\\\\n",
    "            0&0&0&0&0&1&0\\\\\n",
    "            0&0&0&1&0&0&0\\\\\n",
    "            0&0&0&0&0&0&0\n",
    "\\end{bmatrix} \\\\\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{21}&\\frac{1}{7}&\\frac{1}{7}&\\frac{4}{21}&\\frac{1}{7}&\\frac{1}{7}&\\frac{1}{21}\n",
    "\\end{bmatrix}\n",
    ".\n",
    "$$\n",
    "\n",
    "Assim, temos que a fórmula iterativa\n",
    "$$ r_{k+1}(P_i) = \\sum_{P_j \\in B_{P_i}}\\frac{r_k(P_j)}{|P_j|},\\;\\;\\; \\text{para}\\; i =1,2,\\dots,n, $$\n",
    "pode ser representa pela seguinte forma mais compacta:\n",
    "$$\n",
    "(\\pi^t)^{k+1}\n",
    "=\n",
    "(\\pi^t)^kH . \n",
    "$$\n",
    "\n",
    "Contudo, ainda sim temos problemas no cálculo do _PageRank_. Como já foi discutido na seção \"A Matriz Google\", sabemos que a\n",
    "utilização da matriz $H$ trás problemas para o modelo. Portanto, em vez de $\\pi^k$ ser multiplicado por $H$, ele será\n",
    "multiplicado por $G$. Desse modo tem-se,\n",
    "\n",
    "$$\n",
    "(\\pi^t)^{k+1}\n",
    "=\n",
    "(\\pi^t)^kG . \n",
    "$$\n",
    "\n",
    "Uma implicação \"ruim\" do uso da $G$ na equação é que ela não nos dará o \"real\" ranqueamento de cada página. Isso se\n",
    "deve ao fato de $G$ ser o resultado de uma soma entre $S$ e $ee^t$. A matriz $S$ preserva a estrutura de links das\n",
    "páginas da Internet e lida de forma \"especial\" com páginas sem links. Enquanto $ee^t$, que é uma matriz de\n",
    "teleportação aleatória, não preserva a estrutura. Ainda assim é necessário a utilização de $G$ para, pelo menos,\n",
    "obter uma aproximação do _rank_ \"real\" das páginas.\n",
    "\n",
    "Não é possível de se calcular este _rank_ \"real\", utilizando\n",
    "somente $S$. Apenas o uso dela pode acarretar no fenômeno do usuário fictício seguir um caminho periódico\n",
    "previsível entre as páginas, algo visto na seção anterior. Ou, de  forma mais geral, o usuário pode navegar somente\n",
    "num conglomerado de páginas sem ter a possibilidade do mesmo conseguir  \"chegar\" em alguma página fora desse\n",
    "conglomerado. Em qualquer um desses casos, o _PageRank_ das páginas que não pertencem a esse conglomerado iria\n",
    "tender a zero conforme o cálculo dos _ranks_ for sendo executado. O que implica na falha do modelo para o cálculo\n",
    "do _rank_ dessas páginas, algo não desejado.\n",
    "\n",
    "Esperamos que os elementos de $\\pi^k$, $\\pi_i^k = r_k(P_i)$, convirjam para um valor estável depois de várias\n",
    "iterações, isso significa que para um $k$ \"grande\", $\\pi^k \\approx \\pi$, para certo $\\pi$. Assim, podemos escrever\n",
    "que para $k \\rightarrow \\infty$, $\\pi^k = \\pi$ em que $\\pi$, que é chamado de o _vetor do PageRank_, satisfaz \n",
    "$$\n",
    "\\pi^t\n",
    "=\n",
    "\\pi^tG. \n",
    "$$\n",
    " \n",
    "Visto que há condições impostas aos todos $r_k(P_i)$ $i=1,\\dots,n$, o vetor $\\pi^k$ também as terá. Primeiramente, temos que\n",
    "$r_k(P_i) = \\pi_i^k > 0$ o que implica em $\\pi^k > 0$. E também que como $\\sum_{i=1}^n r_k(P_i) = \\sum_{i=1}^n |r_k(P_i)| =\n",
    "1$ isso significa que a norma 1 de $\\pi^k$, $\\|\\pi^k\\|_1$, é igual a 1. Note que $\\pi^k$ se encaixa na definição de um _vetor\n",
    "de distribuição de probabilidade_ enunciada na seção \"Cadeias de Markov\".\n",
    "\n",
    "Deste modo, o problema do _PageRank_ se reduz a encontrar o _vetor de probabilidade_ $\\pi$ que satisfaz as condições $\\pi > 0$,\n",
    "$\\|\\pi\\|_1$ = 1 e também $\\pi^t = \\pi^tG$.\n",
    "\n",
    "Agora que formulamos o problema e sabemos as condições que $\\pi$ deve satisfazer já podemos, então, dado uma matriz $G$ de um\n",
    "conjunto de páginas $P$, encontrar o vetor $\\pi$ que representa o _rank_ de cada uma das páginas utilizando a fórmula\n",
    "$(\\pi^t)^{k+1} = (\\pi^t)^kG$. Contudo, até o momento nada nos garante que o método proposto irá funcionar, isto é, convergir\n",
    "para $\\pi$. Ainda mais, caso funcionasse, poderia haver o caso de existir mais de um vetor que satisfazesse as condições para\n",
    "ser classificado como _Vetor do PageRank_, implicando na existência de mais de um _rank_ para a mesma página, algo não\n",
    "desejado. E, além do mais, pode ainda haver o caso do vetor $\\pi$ simplesmente não existir. \n",
    "\n",
    "Deste modo ficamos com os seguintes questionamentos a respeito do _Vetor do PageRank_ $\\pi$: Dado uma matriz $G$, o vetor\n",
    "$\\pi$ existe? ele é único? o método proposto para seu cálculo sempre converge? \n",
    "\n",
    "Antes de poder começar a responder essas perguntas, precisamos fazer uma pequena pausa no raciocínio para falar sobre\n",
    "autovalores e autovetores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574371f7",
   "metadata": {},
   "source": [
    "## Autovalor e Autovetor\n",
    "\n",
    "Você provavelmente está acostumado com a seguinte definição de autovalor e autovetor: dada uma matriz qualquer $A\n",
    "\\in \\mathbb{R}^{n \\times n}$ e um vetor $v \\in \\mathbb{R}^n$, com $v \\neq 0$, $v$ é autovetor de $A$ associado ao\n",
    "autovalor $\\lambda$ se $$ Av = \\lambda v,$$ para algum $\\lambda \\in \\mathbb{R}$. Uma forma de se calcular os\n",
    "autovalores $\\lambda$ é pelo _polinômio característico_ de $A$, $p(A) = det(A-\\lambda I)$.\n",
    "\n",
    "Porém, mesmo A possuindo apenas entradas reais, as raízes de $p(A)$ podem assumir valores complexos. Assim, se nos\n",
    "restringirmos a valores reais para os autovalores $\\lambda$, é possível que haja a \"perda\" de certos $\\lambda$.\n",
    "Isso implicaria que a \"quantidade\", multiplicidade algébrica, de autovalores de $A$ seja menor do que $n$, sua\n",
    "dimensão, o que implicaria em problemas para nossa análise. Portanto, para que nenhum $\\lambda$ fique de fora, será\n",
    "adotada a seguinte definição de autovalor e autovetor:\n",
    "\n",
    "Dada uma matriz $A \\in \\mathbb{C}^{n \\times n}$ e um vetor $v \\in \\mathbb{C}^n$, com $v \\neq 0$, $v$ é autovetor de\n",
    "$A$ associado ao autovalor $\\lambda$ se $$ Av = \\lambda v,$$ para algum $\\lambda \\in \\mathbb{C}$.\n",
    "\n",
    "O conjunto de autovalores $\\lambda$ que uma matriz $A$ possui será chamado de _espectro_ de A, denotado por\n",
    "$\\sigma(A)$.  O valor absoluto dos maiores $\\lambda$ em módulo, $|\\lambda _i| \\geq |\\lambda _j|$, será chamado de\n",
    "_raio espectral_, denotado por $\\rho(A)$. \n",
    "\n",
    "Uma observação importante é que existem tanto autovetor à direita, $Ax = \\lambda x$, quanto à esquerda, $y^tA =\n",
    "\\lambda y^t$. Os autovetores à direita são os que estamos mais habituados a lidar. Mas não se preocupe, se você\n",
    "entende autovetores à direita você  também entende o à esquerda. É praticamente a mesma ideia. Algo não difícil de\n",
    "provar é que o espectro $\\sigma(A)$ de uma matriz A, o conjunto dos autovalores de uma matriz, dos autovetores à\n",
    "direita e dos à esquerda de $A$ é o mesmo (tente provar você mesmo!), assim, se existem autovetores à direita para\n",
    "um certo $\\lambda$ irá também existir autovetores à esquerda para o mesmo $\\lambda$. Porém há sim, certas\n",
    "diferenças entre ambos, como por exemplo que se um vetor $v$ é autovetor à direita associado a um autovalor\n",
    "$\\lambda$ isso não implica que $v^t$ também será autovetor à esquerda associado a $\\lambda$.\n",
    "\n",
    "Agora que abordamos o conceito de autovalores e autovetores, vamos voltar a discussão a respeito do problema do _PageRank_.\n",
    "Relembrando, queremos encontrar um _vetor de densidade de probabilidade_ $\\pi$, chamado de _Vetor do PageRank_, tal que $\\pi^t\n",
    "= \\pi^tG$, $\\pi$ seja positivo e tenha norma 1 igual a 1. Observando novamente sob novos olhares as condições de $\\pi$, mais\n",
    "especificamente a primeira mencionada, vemos que $\\pi$ nada mais é que um autovetor a esquerda de $G$ associado ao autovalor\n",
    "$\\lambda = 1$. Deste modo, o problema do _PageRank_ também é um problema de autovalor e autovetor.\n",
    "\n",
    "Agora que conhecemos mais um característica de $\\pi$, podemos começar a encontrar respostas para os questionamentos a\n",
    "respeito do mesmo. Primeiramente, para discutir sobre a existência e unicidade, utilizaremos um teorema importante na área de\n",
    "Álgebra Linear, o chamado Teorema de Perron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1de62a",
   "metadata": {},
   "source": [
    "## Perron\n",
    "\n",
    "O Teorema de Perron infere propriedades de um autovalor específico de uma matriz $A$ que satisfaz apenas duas\n",
    "condições: A é quadrada, $A \\in \\mathbb{R}^{n \\times n}$, e que todos seus elementos sejam positivos, $a_{ij} > 0,\n",
    "i,j = 1,2,\\dots,n$.\n",
    "\n",
    "Dentre todas as afirmações que o teorema diz, as mais importantes para nós são as seguintes: Seja A uma matriz\n",
    "quadrada com apenas entradas positivas. Existe um único autovetor $p$ à direita de $A$, com $p > 0$ e $\\|p\\|_1 =\n",
    "1$, chamado _vetor de Perron_, associado a um autovalor $\\lambda = r > 0$, chamado de _raiz de Perron_, com\n",
    "multiplicidade algébrica igual à 1. Além de $p$ ser único, ele ainda possui a \"unicidade\" de, exceto de seus\n",
    "múltiplos positivos, não haver outros autovetores positivos à direita de $A$, independentemente do autovalor. E,\n",
    "além de tudo isso, $r$ é o maior autovalor em magnitude de $A$, com $|r| = \\rho(A)$. \n",
    "\n",
    "No teorema está sendo apenas descrito o _autovetor de Perron_ à direita tal que $Ap = rp$. Porém ainda assim existe\n",
    "o _autovetor de Perron_ à esquerda, digamos $q$, com as mesmas condições e propriedades de $p$ associado também a\n",
    "_raiz de Perron_ $r$ em que $q^tA = rq^t$.\n",
    "\n",
    "\n",
    "A _matriz Google_ $G$ satisfaz as duas condições iniciais do teorema, assim deve existir um autovalor $\\lambda = r$\n",
    "de $G$ que possui as propriedades descritas acima. O que nos resta agora é encontrá-lo.\n",
    "\n",
    "Note que a multiplicação entre $G$ e $e$, o vetor coluna com todas entradas iguais a 1, representa a soma dos\n",
    "elementos das linhas de $G$, que por construção é igual à 1. Deste modo temos que,  \n",
    "$$ Ge = e. $$\n",
    "\n",
    "Esse resultado implica que o vetor $e$ é um autovetor à direita de $G$ com autovalor $\\lambda = 1$. Sabemos pelo\n",
    "Teorema de Perron que autovetores positivos devem ter como autovalor associado $\\lambda = r$. Como $e> 0$ e tem\n",
    "como autovalor associado $\\lambda = 1$, concluímos que $r = 1$.\n",
    "\n",
    "Agora que sabemos que o _autovalor de Perron_ de $G$ é 1, podemos nos apropriar das propriedades do teorema. Como\n",
    "o _vetor do PageRank_ $\\pi$ satisfaz\n",
    "$$ \\pi^t = \\pi^tG,\\; \\pi > 0 \\; e \\; \\|\\pi\\|_1 = 1,$$\n",
    "temos que $\\pi$ é o _vetor de Perron_ à esquerda de $G$. Assim, utilizando $G$ para se calcular o vetor $\\pi$, sua\n",
    "existência e unicidade são garantidos graças ao Teorema de Perron.\n",
    "\n",
    "Como agora sabemos que o _vetor de probabilidade_ $\\pi$ existe e é único, só nos resta ainda um questionamento: é\n",
    "possível calcular $\\pi$ pelo método iterativo proposto?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa0808a",
   "metadata": {},
   "source": [
    "## Método da Potência\n",
    "\n",
    "O método iterativo proposto na seção Fórmula para o PageRank é um caso específico do Método da Potência. O\n",
    "Método da Potência é uma técnica iterativa usada para determinar o autovalor dominante de uma matriz, isto é, o\n",
    "maior autovalor em magnitude. O método, além de calcular um autovalor, obtém também um autovetor\n",
    "associado.\n",
    "\n",
    "Para poder aplicar o Método da Potência, devemos ter uma matriz $A_{n \\times n}$ diagonalizável (possui $n$\n",
    "autovetores linearmente independentes) com um maior autovalor em magnitude $\\rho(A) = |\\lambda_1|$, de tal forma que\n",
    "$$ \n",
    "|\\lambda_1| > |\\lambda_2| \\geq |\\lambda_3| \\geq \\dots \\geq |\\lambda_n|.  \n",
    "$$\n",
    "Observe que o fato de $|\\lambda_1| > |\\lambda_i|$ para $i = 2, \\dots, n$ impõe que $\\lambda_1 \\in \\mathbb{R}$. Caso\n",
    "a parte imaginária de $\\lambda_1$ fosse diferente de zero, existiria um autovalor conjugado $\\bar{\\lambda}_1 \\neq\n",
    "\\lambda_1$ de tal forma que $|\\bar{\\lambda}_1| = |\\lambda_1|$, que quebraria a hipótese.\n",
    "\n",
    "Seja $\\left\\{\\nu^1, \\nu^{2}, \\nu^{3}, \\dots, \\nu^{n}\\right\\}$ um conjunto de $n$ autovetores linearmente independentes\n",
    "associados aos seus respectivos autovalores $\\lambda_i$. Podemos representar um vetor $x$ qualquer utilizando os\n",
    "autovetores $\\nu^{i}$'s como base. Assim,\n",
    "$$\n",
    "x = \\sum_{i = 1}^n \\alpha_i \\nu^{i}.\n",
    "$$\n",
    "Aplicando a matriz $A$ a $x$ obtemos,\n",
    "$$\n",
    "Ax = \\sum_{i = 1}^n \\alpha_i A\\nu^{i} = \\sum_{i = 1}^n \\lambda_i \\alpha_i \\nu^{i}.\n",
    "$$\n",
    "Aplicando a matriz $A$ a $Ax$ obtemos,\n",
    "$$\n",
    "AAx = A^2x = \\sum_{i = 1}^n \\lambda_i \\alpha_i A\\nu^{i} = \\sum_{i = 1}^n \\lambda_i^2 \\alpha_i \\nu^{i}.\n",
    "$$\n",
    "Fazendo esse mesmo processo $k$ vezes chegamos a,\n",
    "$$\n",
    "AA^{k-1}x = A^kx = \\sum_{i = 1}^n \\lambda_i^{k-1} \\alpha_i A\\nu^{i} = \\sum_{i = 1}^n \\lambda_i^k \\alpha_i \\nu^{i}.\n",
    "$$\n",
    "Fatorando $\\lambda_1^k$ do lado direito da última equação vemos que,\n",
    "$$\n",
    "A^kx = \\lambda_1^k \\sum_{i = 1}^n \\left(\\frac{\\lambda_i}{\\lambda_1}\\right)^k \\alpha_i \\nu^{i},\n",
    "$$\n",
    "e assim, chegamos a,\n",
    "$$\n",
    "A^kx = \\lambda_1^k \\left(\\alpha_1 \\nu^{1} + \\left(\\frac{\\lambda_2}{\\lambda_1}\\right)^k\n",
    "\\alpha_2 \\nu^{2} + \\dots + \\left(\\frac{\\lambda_n}{\\lambda_1}\\right)^k \\alpha_n \\nu^{n} \\right).\n",
    "$$\n",
    "Observe que como $|\\lambda_1| > |\\lambda_i|$ para $i = 2,\\dots,n$, temos que $(\\lambda_i/\\lambda_1) < 1$ e assim,\n",
    "$$\n",
    "\\lim_{k \\to \\infty} \\left(\\frac{\\lambda_i}{\\lambda_1}\\right)^k = 0, \\; \\; \\text{para } i = 2, \\dots, n.\n",
    "$$\n",
    "Portanto conforme aumenta o número de iterações $k$, os termos de $\\lambda_1^k \\sum_{i=1}^{n}\n",
    "\\left(\\frac{\\lambda_i}{\\lambda_1}\\right)^k \\alpha_i \\nu^{i}$ vão tendendo cada vez mais a $\\lambda_1^k \\alpha_1 \\nu^{1}$. Para que\n",
    "possamos tomar o limite de $k$ tendendo ao infinito, devemos tomar o cuidado para que $\\lambda_1^k$ convirja. Para\n",
    "isso, só nos resta a opção $\\lambda_1 = 1$. Caso $|\\lambda_1|$ fosse menor do que 1, teríamos $\\lim_{k \\to \\infty}\n",
    "\\lambda_1^k = 0$, que não nos interessa. Caso $|\\lambda_1| > 1$, o limite $\\lim_{k \\to \\infty}\\lambda_1^k$ iria\n",
    "divergir para o infinito. E se caso $\\lambda_1 = -1$, $\\lambda_1^k$ não convergiria para um valor fixo conforme\n",
    "$k$ aumenta. Assim, assumindo $\\lambda_1 = 1$ e tomando o limite de $k$ tendendo ao infinito, vemos que,\n",
    "$$\n",
    "\\lim_{k \\to \\infty}A^kx =  \\lim_{k \\to \\infty} \\lambda_1^k \\left(\\alpha_1 \\nu^{1} + \\left(\\frac{\\lambda_2}{\\lambda_1}\\right)^k\n",
    "\\alpha_2 \\nu^{2} + \\dots + \\left(\\frac{\\lambda_n}{\\lambda_1}\\right)^k \\alpha_n \\nu^{n} \\right) = \\lim_{k \\to \\infty} \n",
    "\\lambda_1^k \\alpha_1 \\nu^{1}\n",
    "$$\n",
    "e assim,\n",
    "$$\n",
    "\\lim_{k \\to \\infty}A^kx = \\alpha_1 \\nu^{1}.\n",
    "$$\n",
    "\n",
    "Portanto, chegamos à conclusão que, pela equação acima, $A^kx$ tende a um par de autovalor-autovetor, com\n",
    "autovalor $\\lambda = 1$ e autovetor associado $\\nu = \\alpha_1 \\nu^{1}$ sempre que $A$ for diagonalizável e tiver\n",
    "$\\lambda_1 = 1$ como o maior autovalor em módulo. A única forma do limite dar um resultado ``insatisfatório\", é\n",
    "caso $\\alpha_1 = 0$, isto é, a contribuição de $\\nu^{1}$ na representação de $x$ pela base $\\left\\{\\nu^{1},\n",
    "\\nu^{2}, \\dots, \\nu^{n}\\right\\}$ for zero. Contudo, se isso acontecesse, o que é raro, poderíamos apenas usar\n",
    "algum outro $x_0$ que tenha $\\alpha_1 \\neq 0$ em $A^kx_0$ para obter um par autovalor-autovetor com $\\lambda = 1$ e\n",
    "o autovetor associado.\n",
    "\n",
    "Observe que, a velocidade de convergência de $A^k$ para $\\alpha_1\\nu^{1}$ dependerá do valor de\n",
    "$\\left(\\frac{\\lambda_2}{\\lambda_1}\\right)$. Caso $|\\lambda_2| \\approx |\\lambda_1|$, teremos\n",
    "$\\left(\\frac{\\lambda_2}{\\lambda_1}\\right) \\approx 1$, o que implica em uma convergência lenta para\n",
    "$\\alpha_1\\nu^{1}$. E se caso $|\\lambda_2|$ for bem menor que $|\\lambda_1|$, teremos\n",
    "$\\left(\\frac{\\lambda_2}{\\lambda_1}\\right) \\ll 1$, o que implica em uma convergência rápida do método.\n",
    "\n",
    "\n",
    "Desenvolvemos até aqui o Método da Potência para $\\lambda_1 = 1$, que é o caso que justamente nos interessa. No\n",
    "caso que $\\lambda_1 \\neq 1$, o Método da Potência é levemente diferente. Nesse, $A^kx$ é normalizada a cada\n",
    "iteração, com o intuito de não permitir que tende a zero ou divirja. Todavia, a ideia por trás e o resultado final\n",
    "obtido será análogo ao que foi desenvolvido.\n",
    "\n",
    "Olhando de volta para a _matriz Google_ $G$, podemos assumir a propriedade da mesma ser diagonalizável. O fenômeno\n",
    "de uma matriz qualquer não ser diagonalizável é raro, e experimentos numéricos mostram que supor a\n",
    "diagonalizibilidade de $G$ não acarreta em problemas ou contradições. Portanto, é razoável supor essa propriedade. \n",
    "\n",
    "O que nos resta agora é analisar os maiores autovalores em magnitude de $G$ para se certificar que o Método da\n",
    "Potência aplicada a ela converge. Já sabemos da seção Perron que $\\lambda_1 = 1 = \\rho(G)$, que implica que $1 \\geq\n",
    "|\\lambda_2| \\geq \\dots \\geq |\\lambda_n|$.  Porém, deve-se ter $\\lambda_1 = 1$ como o único autovalor no raio\n",
    "espectral, isto é, $1 > |\\lambda_i|$ para $i = 2,3, \\dots, n$. É possível provar, no entanto, não o faremos aqui,\n",
    "que o parâmetro $\\alpha \\in (0,1)$ da equação que define $G$,\n",
    "$$\n",
    "G = \\alpha S + (1 - \\alpha)\\frac{1}{n} ee^t, \n",
    "$$\n",
    "é o segundo maior autovalor em magnitude de $G$. Como $\\lambda_2 = \\alpha < 1$, temos que $1 > \\alpha \\geq\n",
    "|\\lambda_3| \\geq \\dots \\geq |\\lambda_n|$.\n",
    "\n",
    "Como $G$ satisfaz ambas as hipóteses para a convergência do Método da Potência, temos que o limite $\\lim_{k \\to\n",
    "\\infty} G^k x$ converge, com uma velocidade dependendo de $\\left(\\frac{\\alpha}{1}\\right)$, para um autovetor $\\nu$\n",
    "associado ao autovalor $\\lambda = 1$. Assim,\n",
    "\n",
    "$$\n",
    "\\lim_{k \\to \\infty} G^kx = \\nu.\n",
    "$$\n",
    "\n",
    "Como $\\pi$ e $\\nu$ são autovetores associados a $\\lambda = 1$ que, devido ao Teorema de Perron, possui\n",
    "multiplicidade algébrica igual a 1, sabemos que $\\nu$ é um múltiplo de $\\pi$. Devido ao fato de $\\|\\pi\\|_1 = 1$, \n",
    "$\\pi > 0$ e $\\nu$ ser $\\nu > 0$ ou $\\nu < 0$, temos que\n",
    "\n",
    "$$\n",
    "\\pi = \\text{sgn}(\\nu) \\frac{\\nu}{\\|\\nu\\|_1},\n",
    "$$\n",
    "\n",
    "em que $\\text{sgn}(\\nu)$ = 1 caso $\\nu > 0$ e $\\text{sgn}(\\nu)$ = -1 caso $\\nu < 0$. Assim, chegamos na conclusão\n",
    "que o _vetor do PageRank_ $\\pi$ é calculável pelo Método da Potência aplicado a $G$.\n",
    "\n",
    "Como, agora, respondemos todas nossas dúvidas sobre $\\lambda = 1$ ser autovalor de $G$, sobre a existência e\n",
    "unicidade de $\\pi$ e também vimos que o método proposto funciona, vamos calcular o _vetor do PageRank_ $\\pi$ para\n",
    "alguns casos.\n",
    "\n",
    "\n",
    "[$\\dots \\rightarrow$ Método da Potência $\\rightarrow$ Exemplos]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0739cdc5",
   "metadata": {},
   "source": [
    "# Exemplos\n",
    "\n",
    "Nessa seção, calcularemos o _vetor do PageRank_ $\\pi$ para alguns casos. O cálculo será feito pelo seguinte código\n",
    "em Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "602c4317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "número de iterações:  39\n",
      "vetor PageRank:\n",
      "[0.05352337 0.07342271 0.09033715 0.25251666 0.24256699 0.23410976\n",
      " 0.05352337]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def Vetor_PageRank(H, alpha, epsilon):\n",
    "    H = H.astype(float)\n",
    "    n = np.size(H, 0)\n",
    "    a = np.sum(H, axis = 1) #justificar\n",
    "    for i in range(n):\n",
    "        if a[i] == 0:\n",
    "            a[i] = 1\n",
    "        else:\n",
    "            H[i] = H[i]/a[i]\n",
    "            a[i] = 0\n",
    "    pi      = np.ones(n)/n\n",
    "    k       = 0\n",
    "    residuo = 1\n",
    "    while residuo >= epsilon:\n",
    "        prevpi  = np.array(pi)\n",
    "        k       = k + 1\n",
    "        pi      = np.array(alpha*pi@H + (alpha*(pi@a)+1-alpha)*(np.ones(n)/n)) #justificar\n",
    "        residuo = np.linalg.norm(pi - prevpi, ord = 1)\n",
    "    print(\"número de iterações: \",k)\n",
    "    print(\"vetor PageRank:\")\n",
    "    print(pi)\n",
    "    print(np.linalg.norm(pi, ord=1))\n",
    "\n",
    "H = np.array([[0,1,0,0,0,0,0],[0,0,1,0,0,0,0],[1,0,0,1,0,0,1],[0,0,0,0,1,0,0],[0,0,0,0,0,1,0],[0,0,0,1,0,0,0,],[0,0,0,0,0,0,0]])\n",
    "#Vetor_PageRank(np.array([[0,1/2,1/2,0,0,0],[0,0,0,0,0,0],[1/3,1/3,0,0,1/3,0],[0,0,0,0,1/2,1/2],[0,0,0,1/2,0,1/2],[0,0,0,1,0,0]]),0.9,10**(-6))\n",
    "Vetor_PageRank(H, 0.85, 10**(-7))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e670bfa5",
   "metadata": {},
   "source": [
    "Voltando ao grafo utilizado na seção ``A Matriz Google\", que, para recordação, é dado por \n",
    "\n",
    "![Grafo](Imagens/Grafo_1.png) \n",
    "\n",
    "calcularemos o _rank_ de cada página de $P = \\left(P_1, P_2, P_3, P_4, P_5, P_6, P_7\\right)$.\n",
    "\n",
    "Como já trabalhamos nesse exemplo, já sabemos como é dado a matriz $S$,\n",
    "$$ \n",
    "S = \\begin{bmatrix}\n",
    "            0&1&0&0&0&0&0 \\\\\n",
    "            0&0&1&0&0&0&0\\\\\n",
    "            1/3&0&0&1/3&0&0&1/3\\\\\n",
    "            0&0&0&0&1&0&0\\\\\n",
    "            0&0&0&0&0&1&0\\\\\n",
    "            0&0&0&1&0&0&0\\\\\n",
    "            1/7&1/7&1/7&1/7&1/7&1/7&1/7 \n",
    "\\end{bmatrix}. \n",
    "$$\n",
    "Utilizando o parâmetro $\\alpha$ como sendo $0.85$, temos que $G$ é \n",
    "$$\n",
    "        G = \\alpha S + (1 - \\alpha)\\frac{1}{n} ee^t, \\\\ \n",
    "          = 0.85\n",
    "        \\begin{bmatrix}\n",
    "            0&1&0&0&0&0&0 \\\\\n",
    "            0&0&1&0&0&0&0\\\\\n",
    "            1/3&0&0&1/3&0&0&1/3\\\\\n",
    "            0&0&0&0&1&0&0\\\\\n",
    "            0&0&0&0&0&1&0\\\\\n",
    "            0&0&0&1&0&0&0\\\\\n",
    "            1/7&1/7&1/7&1/7&1/7&1/7&1/7 \n",
    "        \\end{bmatrix}\n",
    "        + 0.15\n",
    "        \\begin{bmatrix} 1/7&1/7&1/7&1/7&1/7&1/7&1/7\\\\\n",
    "            1/7&1/7&1/7&1/7&1/7&1/7&1/7\\\\\n",
    "            1/7&1/7&1/7&1/7&1/7&1/7&1/7\\\\\n",
    "            1/7&1/7&1/7&1/7&1/7&1/7&1/7\\\\\n",
    "            1/7&1/7&1/7&1/7&1/7&1/7&1/7\\\\\n",
    "            1/7&1/7&1/7&1/7&1/7&1/7&1/7\\\\\n",
    "            1/7&1/7&1/7&1/7&1/7&1/7&1/7\n",
    "        \\end{bmatrix}.\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "Cell": {
   "cm_config": {
    "lineWrapping": true
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
